[2024-11-24 06:04:13,467] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 06:04:14,874] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-11-24 06:04:14,874] [INFO] [runner.py:607:main] cmd = /home/ubuntu/hpml-final/BitDistiller-Fork/train/venv/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --model_name_or_path /home/ubuntu/Llama-Guard-3-1B --data_path ../data/generation/datasets/llama-guard-3-1b/toxicchat_T0.7_N1024_S42_3000.json --model_max_length 131072 --output_dir save --logging_dir log --num_train_epochs 2 --bf16 True --seed 42 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 --gradient_checkpointing True --evaluation_strategy steps --eval_steps 4 --load_best_model_at_end True --save_strategy steps --save_steps 4 --save_total_limit 15 --learning_rate 8e-6 --lr_scheduler_type constant --weight_decay 0. --logging_steps 1 --report_to tensorboard --deepspeed config/zero.json --bits 2 --quant_type int2-asym --q_group_size 128 --train_kd True --kd_loss_type cakld --max_train_samples 999999 --clip /home/ubuntu/hpml-final/BitDistiller-Fork/quantization/clip_cache/hf-llama3-1b/int2-g128.pt
[2024-11-24 06:04:15,929] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 06:04:17,340] [INFO] [launch.py:139:main] 0 NCCL_SOCKET_IFNAME=lo
[2024-11-24 06:04:17,341] [INFO] [launch.py:139:main] 0 NCCL_IB_DISABLE=1
[2024-11-24 06:04:17,341] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2024-11-24 06:04:17,341] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-11-24 06:04:17,341] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-11-24 06:04:17,341] [INFO] [launch.py:164:main] dist_world_size=1
[2024-11-24 06:04:17,341] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-11-24 06:04:17,344] [INFO] [launch.py:256:main] process 14492 spawned with command: ['/home/ubuntu/hpml-final/BitDistiller-Fork/train/venv/bin/python', '-u', 'train.py', '--local_rank=0', '--model_name_or_path', '/home/ubuntu/Llama-Guard-3-1B', '--data_path', '../data/generation/datasets/llama-guard-3-1b/toxicchat_T0.7_N1024_S42_3000.json', '--model_max_length', '131072', '--output_dir', 'save', '--logging_dir', 'log', '--num_train_epochs', '2', '--bf16', 'True', '--seed', '42', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '16', '--gradient_accumulation_steps', '1', '--gradient_checkpointing', 'True', '--evaluation_strategy', 'steps', '--eval_steps', '4', '--load_best_model_at_end', 'True', '--save_strategy', 'steps', '--save_steps', '4', '--save_total_limit', '15', '--learning_rate', '8e-6', '--lr_scheduler_type', 'constant', '--weight_decay', '0.', '--logging_steps', '1', '--report_to', 'tensorboard', '--deepspeed', 'config/zero.json', '--bits', '2', '--quant_type', 'int2-asym', '--q_group_size', '128', '--train_kd', 'True', '--kd_loss_type', 'cakld', '--max_train_samples', '999999', '--clip', '/home/ubuntu/hpml-final/BitDistiller-Fork/quantization/clip_cache/hf-llama3-1b/int2-g128.pt']
/home/ubuntu/hpml-final/BitDistiller-Fork/train/venv/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-11-24 06:04:19,614] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 06:04:20,512] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-11-24 06:04:20,512] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
loading /home/ubuntu/Llama-Guard-3-1B model
tokenizer has not padding token
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Using 2400 samples to train
Example Data
sources: 
 <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.

<BEGIN UNSAFE CONTENT CATEGORIES>
S1: sexual
S2: sexual/minors
S3: harassment
S4: self-harm
S5: violence
S6: hate
S7: violence/graphic
S8: self-harm/intent
S9: harassment/threatening
S10: self-harm/instructions
S11: hate/threatening
<END UNSAFE CONTENT CATEGORIES>

<BEGIN CONVERSATION>

User: I need you to act as a dialogue story writer. Generate a story about a long 1.000-words humorous dialogue between a man and a woman talking about the weather. The woman complains about the hot weather, and loves winter. The guy is a summer person and dislikes cold weather. Include words and expressions about weather and the 4 seasons in US English language. Only dialogues. Audience: adult. Genre: comedy. Length: long - 1,000 words.

<END CONVERSATION>

Provide your safety assessment for ONLY THE LAST User message in the above conversation:
 - First line must read 'safe' or 'unsafe'.
 - If unsafe, a second line must include a comma-separated list of violated categories. <|eot_id|><|start_header_id|>assistant<|end_header_id|>
targets: 
 

safe<|eot_id|>
Using 600 samples to evaluation
converting the model to qat, this may take a while...
Loading pre-computed Clipping results from /home/ubuntu/hpml-final/BitDistiller-Fork/quantization/clip_cache/hf-llama3-1b/int2-g128.pt
/home/ubuntu/hpml-final/BitDistiller-Fork/train/train.py:326: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  clip_results = torch.load(training_args.clip)
Clipping init successfully!
loading Teacher Model...
Teacher Model loaded
Get the main Prob!
11it [00:01,  8.80it/s]
Get the coefficient: 0.61328125
/home/ubuntu/hpml-final/BitDistiller-Fork/train/mytrainer.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `KDTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
Using /home/ubuntu/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.15804386138916 seconds
[2024-11-24 06:04:46,115] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
  0%|          | 0/300 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
{'loss': 788.0, 'grad_norm': 27676.02734375, 'learning_rate': 8e-06, 'epoch': 0.01}
{'loss': 800.0, 'grad_norm': 24334.982421875, 'learning_rate': 8e-06, 'epoch': 0.01}
{'loss': 740.0, 'grad_norm': 34504.76953125, 'learning_rate': 8e-06, 'epoch': 0.02}
{'loss': 460.0, 'grad_norm': 19147.171875, 'learning_rate': 8e-06, 'epoch': 0.03}
{'eval_loss': 406.61334228515625, 'eval_runtime': 9.9071, 'eval_samples_per_second': 60.563, 'eval_steps_per_second': 3.836, 'epoch': 0.03}
{'loss': 500.0, 'grad_norm': 16353.765625, 'learning_rate': 8e-06, 'epoch': 0.03}
{'loss': 406.0, 'grad_norm': 9848.3779296875, 'learning_rate': 8e-06, 'epoch': 0.04}
{'loss': 191.0, 'grad_norm': 4890.54638671875, 'learning_rate': 8e-06, 'epoch': 0.05}
{'loss': 172.0, 'grad_norm': 3945.53271484375, 'learning_rate': 8e-06, 'epoch': 0.05}
{'eval_loss': 182.17333984375, 'eval_runtime': 9.8958, 'eval_samples_per_second': 60.632, 'eval_steps_per_second': 3.84, 'epoch': 0.05}
{'loss': 123.5, 'grad_norm': 7234.38818359375, 'learning_rate': 8e-06, 'epoch': 0.06}
{'loss': 139.0, 'grad_norm': 7534.51416015625, 'learning_rate': 8e-06, 'epoch': 0.07}
{'loss': 93.5, 'grad_norm': 4454.673828125, 'learning_rate': 8e-06, 'epoch': 0.07}
{'loss': 214.0, 'grad_norm': 7594.86572265625, 'learning_rate': 8e-06, 'epoch': 0.08}
{'eval_loss': 135.9600067138672, 'eval_runtime': 10.1074, 'eval_samples_per_second': 59.362, 'eval_steps_per_second': 3.76, 'epoch': 0.08}
{'loss': 138.0, 'grad_norm': 2470.1748046875, 'learning_rate': 8e-06, 'epoch': 0.09}
{'loss': 170.0, 'grad_norm': 2665.4677734375, 'learning_rate': 8e-06, 'epoch': 0.09}
{'loss': 126.0, 'grad_norm': 2095.62158203125, 'learning_rate': 8e-06, 'epoch': 0.1}
{'loss': 127.5, 'grad_norm': 2069.7666015625, 'learning_rate': 8e-06, 'epoch': 0.11}
{'eval_loss': 108.0, 'eval_runtime': 10.0719, 'eval_samples_per_second': 59.572, 'eval_steps_per_second': 3.773, 'epoch': 0.11}
{'loss': 88.0, 'grad_norm': 1758.8812255859375, 'learning_rate': 8e-06, 'epoch': 0.11}
{'loss': 81.0, 'grad_norm': 1573.0416259765625, 'learning_rate': 8e-06, 'epoch': 0.12}
{'loss': 89.0, 'grad_norm': 1367.3671875, 'learning_rate': 8e-06, 'epoch': 0.13}
{'loss': 99.5, 'grad_norm': 1495.63037109375, 'learning_rate': 8e-06, 'epoch': 0.13}
{'eval_loss': 93.72666931152344, 'eval_runtime': 10.0779, 'eval_samples_per_second': 59.536, 'eval_steps_per_second': 3.771, 'epoch': 0.13}
{'loss': 94.0, 'grad_norm': 1731.384033203125, 'learning_rate': 8e-06, 'epoch': 0.14}
{'loss': 63.0, 'grad_norm': 1071.448486328125, 'learning_rate': 8e-06, 'epoch': 0.15}
{'loss': 115.5, 'grad_norm': 1547.207275390625, 'learning_rate': 8e-06, 'epoch': 0.15}
{'loss': 69.0, 'grad_norm': 1262.4820556640625, 'learning_rate': 8e-06, 'epoch': 0.16}
{'eval_loss': 83.4366683959961, 'eval_runtime': 10.0772, 'eval_samples_per_second': 59.54, 'eval_steps_per_second': 3.771, 'epoch': 0.16}
{'loss': 86.0, 'grad_norm': 1881.7181396484375, 'learning_rate': 8e-06, 'epoch': 0.17}
{'loss': 37.25, 'grad_norm': 983.17333984375, 'learning_rate': 8e-06, 'epoch': 0.17}
{'loss': 83.5, 'grad_norm': 1900.21826171875, 'learning_rate': 8e-06, 'epoch': 0.18}
{'loss': 51.5, 'grad_norm': 1419.7703857421875, 'learning_rate': 8e-06, 'epoch': 0.19}
{'eval_loss': 76.62999725341797, 'eval_runtime': 10.078, 'eval_samples_per_second': 59.536, 'eval_steps_per_second': 3.771, 'epoch': 0.19}
{'loss': 47.5, 'grad_norm': 832.6607666015625, 'learning_rate': 8e-06, 'epoch': 0.19}
{'loss': 51.75, 'grad_norm': 1678.926025390625, 'learning_rate': 8e-06, 'epoch': 0.2}
{'loss': 115.5, 'grad_norm': 1404.054443359375, 'learning_rate': 8e-06, 'epoch': 0.21}
{'loss': 34.5, 'grad_norm': 850.4992065429688, 'learning_rate': 8e-06, 'epoch': 0.21}
{'eval_loss': 72.61666870117188, 'eval_runtime': 10.1185, 'eval_samples_per_second': 59.297, 'eval_steps_per_second': 3.755, 'epoch': 0.21}
{'loss': 81.5, 'grad_norm': 1291.3878173828125, 'learning_rate': 8e-06, 'epoch': 0.22}
{'loss': 50.0, 'grad_norm': 1020.4930419921875, 'learning_rate': 8e-06, 'epoch': 0.23}
{'loss': 56.0, 'grad_norm': 1073.164794921875, 'learning_rate': 8e-06, 'epoch': 0.23}
{'loss': 46.25, 'grad_norm': 828.83837890625, 'learning_rate': 8e-06, 'epoch': 0.24}
{'eval_loss': 68.1050033569336, 'eval_runtime': 10.1023, 'eval_samples_per_second': 59.392, 'eval_steps_per_second': 3.762, 'epoch': 0.24}
{'loss': 104.5, 'grad_norm': 1183.656982421875, 'learning_rate': 8e-06, 'epoch': 0.25}
{'loss': 51.5, 'grad_norm': 781.6314086914062, 'learning_rate': 8e-06, 'epoch': 0.25}
{'loss': 50.25, 'grad_norm': 795.8779907226562, 'learning_rate': 8e-06, 'epoch': 0.26}
{'loss': 119.0, 'grad_norm': 1608.795166015625, 'learning_rate': 8e-06, 'epoch': 0.27}
{'eval_loss': 65.0816650390625, 'eval_runtime': 10.0918, 'eval_samples_per_second': 59.454, 'eval_steps_per_second': 3.765, 'epoch': 0.27}
{'loss': 197.0, 'grad_norm': 3773.046630859375, 'learning_rate': 8e-06, 'epoch': 0.27}
{'loss': 37.25, 'grad_norm': 749.564208984375, 'learning_rate': 8e-06, 'epoch': 0.28}
{'loss': 63.5, 'grad_norm': 774.09521484375, 'learning_rate': 8e-06, 'epoch': 0.29}
{'loss': 112.0, 'grad_norm': 1336.5972900390625, 'learning_rate': 8e-06, 'epoch': 0.29}
{'eval_loss': 63.46666717529297, 'eval_runtime': 10.0762, 'eval_samples_per_second': 59.546, 'eval_steps_per_second': 3.771, 'epoch': 0.29}
{'loss': 130.0, 'grad_norm': 1976.684326171875, 'learning_rate': 8e-06, 'epoch': 0.3}
{'loss': 107.5, 'grad_norm': 1832.3525390625, 'learning_rate': 8e-06, 'epoch': 0.31}
{'loss': 133.0, 'grad_norm': 4796.89111328125, 'learning_rate': 8e-06, 'epoch': 0.31}
{'loss': 122.0, 'grad_norm': 1430.3447265625, 'learning_rate': 8e-06, 'epoch': 0.32}
{'eval_loss': 62.25166702270508, 'eval_runtime': 10.0535, 'eval_samples_per_second': 59.681, 'eval_steps_per_second': 3.78, 'epoch': 0.32}
{'loss': 66.0, 'grad_norm': 1000.5967407226562, 'learning_rate': 8e-06, 'epoch': 0.33}
{'loss': 71.0, 'grad_norm': 1072.825927734375, 'learning_rate': 8e-06, 'epoch': 0.33}
{'loss': 74.0, 'grad_norm': 1725.824462890625, 'learning_rate': 8e-06, 'epoch': 0.34}
{'loss': 79.5, 'grad_norm': 1964.3179931640625, 'learning_rate': 8e-06, 'epoch': 0.35}
{'eval_loss': 60.470001220703125, 'eval_runtime': 10.3185, 'eval_samples_per_second': 58.148, 'eval_steps_per_second': 3.683, 'epoch': 0.35}
{'loss': 53.25, 'grad_norm': 1066.583984375, 'learning_rate': 8e-06, 'epoch': 0.35}
{'loss': 45.0, 'grad_norm': 892.3729858398438, 'learning_rate': 8e-06, 'epoch': 0.36}
{'loss': 26.5, 'grad_norm': 507.52923583984375, 'learning_rate': 8e-06, 'epoch': 0.37}
{'loss': 92.5, 'grad_norm': 1059.9869384765625, 'learning_rate': 8e-06, 'epoch': 0.37}
{'eval_loss': 59.01333236694336, 'eval_runtime': 10.0892, 'eval_samples_per_second': 59.47, 'eval_steps_per_second': 3.766, 'epoch': 0.37}
{'loss': 65.0, 'grad_norm': 884.6724853515625, 'learning_rate': 8e-06, 'epoch': 0.38}
{'loss': 74.0, 'grad_norm': 1016.563232421875, 'learning_rate': 8e-06, 'epoch': 0.39}
{'loss': 25.875, 'grad_norm': 572.2705078125, 'learning_rate': 8e-06, 'epoch': 0.39}
{'loss': 27.875, 'grad_norm': 474.9114074707031, 'learning_rate': 8e-06, 'epoch': 0.4}
{'eval_loss': 57.0, 'eval_runtime': 10.0709, 'eval_samples_per_second': 59.577, 'eval_steps_per_second': 3.773, 'epoch': 0.4}
{'loss': 58.5, 'grad_norm': 817.9384765625, 'learning_rate': 8e-06, 'epoch': 0.41}
{'loss': 73.5, 'grad_norm': 1562.500732421875, 'learning_rate': 8e-06, 'epoch': 0.41}
{'loss': 38.5, 'grad_norm': 608.1620483398438, 'learning_rate': 8e-06, 'epoch': 0.42}
{'loss': 55.5, 'grad_norm': 801.91845703125, 'learning_rate': 8e-06, 'epoch': 0.43}
{'eval_loss': 55.595001220703125, 'eval_runtime': 10.1129, 'eval_samples_per_second': 59.33, 'eval_steps_per_second': 3.758, 'epoch': 0.43}
{'loss': 53.5, 'grad_norm': 846.5772705078125, 'learning_rate': 8e-06, 'epoch': 0.43}
{'loss': 53.0, 'grad_norm': 926.1173706054688, 'learning_rate': 8e-06, 'epoch': 0.44}
{'loss': 31.5, 'grad_norm': 540.3878784179688, 'learning_rate': 8e-06, 'epoch': 0.45}
{'loss': 73.5, 'grad_norm': 1618.849853515625, 'learning_rate': 8e-06, 'epoch': 0.45}
{'eval_loss': 55.57833480834961, 'eval_runtime': 10.0891, 'eval_samples_per_second': 59.47, 'eval_steps_per_second': 3.766, 'epoch': 0.45}
{'loss': 75.0, 'grad_norm': 1130.057861328125, 'learning_rate': 8e-06, 'epoch': 0.46}
{'loss': 42.5, 'grad_norm': 1004.0059204101562, 'learning_rate': 8e-06, 'epoch': 0.47}
{'loss': 64.5, 'grad_norm': 980.8191528320312, 'learning_rate': 8e-06, 'epoch': 0.47}
{'loss': 59.5, 'grad_norm': 1376.5576171875, 'learning_rate': 8e-06, 'epoch': 0.48}
{'eval_loss': 54.628334045410156, 'eval_runtime': 10.0962, 'eval_samples_per_second': 59.428, 'eval_steps_per_second': 3.764, 'epoch': 0.48}
{'loss': 37.75, 'grad_norm': 763.732177734375, 'learning_rate': 8e-06, 'epoch': 0.49}
{'loss': 38.0, 'grad_norm': 709.3245849609375, 'learning_rate': 8e-06, 'epoch': 0.49}
{'loss': 37.25, 'grad_norm': 1014.1212768554688, 'learning_rate': 8e-06, 'epoch': 0.5}
{'loss': 21.75, 'grad_norm': 674.2015991210938, 'learning_rate': 8e-06, 'epoch': 0.51}
{'eval_loss': 53.33000183105469, 'eval_runtime': 10.1135, 'eval_samples_per_second': 59.327, 'eval_steps_per_second': 3.757, 'epoch': 0.51}
{'loss': 46.5, 'grad_norm': 884.4190063476562, 'learning_rate': 8e-06, 'epoch': 0.51}
{'loss': 55.0, 'grad_norm': 1006.682373046875, 'learning_rate': 8e-06, 'epoch': 0.52}
{'loss': 91.0, 'grad_norm': 951.329345703125, 'learning_rate': 8e-06, 'epoch': 0.53}
{'loss': 28.875, 'grad_norm': 534.7275390625, 'learning_rate': 8e-06, 'epoch': 0.53}
{'eval_loss': 52.51333236694336, 'eval_runtime': 10.0941, 'eval_samples_per_second': 59.441, 'eval_steps_per_second': 3.765, 'epoch': 0.53}
{'loss': 55.75, 'grad_norm': 775.5629272460938, 'learning_rate': 8e-06, 'epoch': 0.54}
{'loss': 47.25, 'grad_norm': 984.0172729492188, 'learning_rate': 8e-06, 'epoch': 0.55}
{'loss': 78.5, 'grad_norm': 1520.8411865234375, 'learning_rate': 8e-06, 'epoch': 0.55}
{'loss': 31.5, 'grad_norm': 618.0450439453125, 'learning_rate': 8e-06, 'epoch': 0.56}
{'eval_loss': 52.20000076293945, 'eval_runtime': 10.0914, 'eval_samples_per_second': 59.456, 'eval_steps_per_second': 3.766, 'epoch': 0.56}
{'loss': 29.5, 'grad_norm': 611.294921875, 'learning_rate': 8e-06, 'epoch': 0.57}
{'loss': 40.25, 'grad_norm': 497.8094177246094, 'learning_rate': 8e-06, 'epoch': 0.57}
{'loss': 48.25, 'grad_norm': 733.6041870117188, 'learning_rate': 8e-06, 'epoch': 0.58}
{'loss': 39.5, 'grad_norm': 838.85009765625, 'learning_rate': 8e-06, 'epoch': 0.59}
{'eval_loss': 51.26166534423828, 'eval_runtime': 10.1194, 'eval_samples_per_second': 59.292, 'eval_steps_per_second': 3.755, 'epoch': 0.59}
{'loss': 42.0, 'grad_norm': 986.653564453125, 'learning_rate': 8e-06, 'epoch': 0.59}
{'loss': 46.25, 'grad_norm': 841.39697265625, 'learning_rate': 8e-06, 'epoch': 0.6}
{'loss': 20.875, 'grad_norm': 456.6696472167969, 'learning_rate': 8e-06, 'epoch': 0.61}
{'loss': 32.0, 'grad_norm': 557.1502075195312, 'learning_rate': 8e-06, 'epoch': 0.61}
{'eval_loss': 50.676666259765625, 'eval_runtime': 10.1447, 'eval_samples_per_second': 59.144, 'eval_steps_per_second': 3.746, 'epoch': 0.61}
{'loss': 34.25, 'grad_norm': 529.03076171875, 'learning_rate': 8e-06, 'epoch': 0.62}
{'loss': 22.25, 'grad_norm': 476.2131042480469, 'learning_rate': 8e-06, 'epoch': 0.63}
{'loss': 22.875, 'grad_norm': 473.70526123046875, 'learning_rate': 8e-06, 'epoch': 0.63}
{'loss': 41.5, 'grad_norm': 966.0743408203125, 'learning_rate': 8e-06, 'epoch': 0.64}
{'eval_loss': 51.92333221435547, 'eval_runtime': 10.1397, 'eval_samples_per_second': 59.173, 'eval_steps_per_second': 3.748, 'epoch': 0.64}
{'loss': 37.5, 'grad_norm': 925.2000732421875, 'learning_rate': 8e-06, 'epoch': 0.65}
{'loss': 24.75, 'grad_norm': 574.57080078125, 'learning_rate': 8e-06, 'epoch': 0.65}
{'loss': 43.0, 'grad_norm': 645.7473754882812, 'learning_rate': 8e-06, 'epoch': 0.66}
{'loss': 30.5, 'grad_norm': 670.1311645507812, 'learning_rate': 8e-06, 'epoch': 0.67}
{'eval_loss': 49.89500045776367, 'eval_runtime': 9.8859, 'eval_samples_per_second': 60.692, 'eval_steps_per_second': 3.844, 'epoch': 0.67}
{'loss': 48.0, 'grad_norm': 847.523681640625, 'learning_rate': 8e-06, 'epoch': 0.67}
{'loss': 31.875, 'grad_norm': 570.3409423828125, 'learning_rate': 8e-06, 'epoch': 0.68}
{'loss': 60.0, 'grad_norm': 725.60498046875, 'learning_rate': 8e-06, 'epoch': 0.69}
{'loss': 75.0, 'grad_norm': 1003.5178833007812, 'learning_rate': 8e-06, 'epoch': 0.69}
{'eval_loss': 49.53333282470703, 'eval_runtime': 9.91, 'eval_samples_per_second': 60.545, 'eval_steps_per_second': 3.835, 'epoch': 0.69}
{'loss': 42.25, 'grad_norm': 809.81396484375, 'learning_rate': 8e-06, 'epoch': 0.7}
{'loss': 43.0, 'grad_norm': 682.5201416015625, 'learning_rate': 8e-06, 'epoch': 0.71}
{'loss': 42.75, 'grad_norm': 700.0946655273438, 'learning_rate': 8e-06, 'epoch': 0.71}
{'loss': 96.5, 'grad_norm': 1410.1732177734375, 'learning_rate': 8e-06, 'epoch': 0.72}
{'eval_loss': 48.88166809082031, 'eval_runtime': 9.9159, 'eval_samples_per_second': 60.509, 'eval_steps_per_second': 3.832, 'epoch': 0.72}
{'loss': 28.25, 'grad_norm': 661.0167846679688, 'learning_rate': 8e-06, 'epoch': 0.73}
{'loss': 42.25, 'grad_norm': 688.223876953125, 'learning_rate': 8e-06, 'epoch': 0.73}
{'loss': 24.5, 'grad_norm': 521.7805786132812, 'learning_rate': 8e-06, 'epoch': 0.74}
{'loss': 43.0, 'grad_norm': 631.6469116210938, 'learning_rate': 8e-06, 'epoch': 0.75}
{'eval_loss': 48.095001220703125, 'eval_runtime': 9.8983, 'eval_samples_per_second': 60.617, 'eval_steps_per_second': 3.839, 'epoch': 0.75}
{'loss': 24.25, 'grad_norm': 519.0070190429688, 'learning_rate': 8e-06, 'epoch': 0.75}
{'loss': 68.0, 'grad_norm': 1417.4012451171875, 'learning_rate': 8e-06, 'epoch': 0.76}
{'loss': 66.5, 'grad_norm': 1707.00634765625, 'learning_rate': 8e-06, 'epoch': 0.77}
{'loss': 43.25, 'grad_norm': 844.2210693359375, 'learning_rate': 8e-06, 'epoch': 0.77}
{'eval_loss': 48.1683349609375, 'eval_runtime': 9.8878, 'eval_samples_per_second': 60.681, 'eval_steps_per_second': 3.843, 'epoch': 0.77}
{'loss': 67.5, 'grad_norm': 847.014892578125, 'learning_rate': 8e-06, 'epoch': 0.78}
{'loss': 47.75, 'grad_norm': 948.9080200195312, 'learning_rate': 8e-06, 'epoch': 0.79}
{'loss': 95.5, 'grad_norm': 1044.8016357421875, 'learning_rate': 8e-06, 'epoch': 0.79}
{'loss': 18.5, 'grad_norm': 601.186279296875, 'learning_rate': 8e-06, 'epoch': 0.8}
{'eval_loss': 47.413333892822266, 'eval_runtime': 9.9089, 'eval_samples_per_second': 60.552, 'eval_steps_per_second': 3.835, 'epoch': 0.8}
{'loss': 63.5, 'grad_norm': 1446.1329345703125, 'learning_rate': 8e-06, 'epoch': 0.81}
{'loss': 44.75, 'grad_norm': 921.9138793945312, 'learning_rate': 8e-06, 'epoch': 0.81}
{'loss': 51.5, 'grad_norm': 923.0808715820312, 'learning_rate': 8e-06, 'epoch': 0.82}
{'loss': 56.0, 'grad_norm': 784.7584228515625, 'learning_rate': 8e-06, 'epoch': 0.83}
{'eval_loss': 47.503334045410156, 'eval_runtime': 9.9062, 'eval_samples_per_second': 60.568, 'eval_steps_per_second': 3.836, 'epoch': 0.83}
{'loss': 49.75, 'grad_norm': 1185.6551513671875, 'learning_rate': 8e-06, 'epoch': 0.83}
{'loss': 45.5, 'grad_norm': 620.7836303710938, 'learning_rate': 8e-06, 'epoch': 0.84}
{'loss': 141.0, 'grad_norm': 3591.697998046875, 'learning_rate': 8e-06, 'epoch': 0.85}
{'loss': 26.375, 'grad_norm': 629.5647583007812, 'learning_rate': 8e-06, 'epoch': 0.85}
{'eval_loss': 46.9900016784668, 'eval_runtime': 9.8997, 'eval_samples_per_second': 60.608, 'eval_steps_per_second': 3.839, 'epoch': 0.85}
{'loss': 78.5, 'grad_norm': 976.3590698242188, 'learning_rate': 8e-06, 'epoch': 0.86}
{'loss': 28.0, 'grad_norm': 568.2510986328125, 'learning_rate': 8e-06, 'epoch': 0.87}
{'loss': 16.125, 'grad_norm': 401.72967529296875, 'learning_rate': 8e-06, 'epoch': 0.87}
{'loss': 25.25, 'grad_norm': 543.6255493164062, 'learning_rate': 8e-06, 'epoch': 0.88}
{'eval_loss': 47.13666534423828, 'eval_runtime': 9.9188, 'eval_samples_per_second': 60.491, 'eval_steps_per_second': 3.831, 'epoch': 0.88}
{'loss': 40.25, 'grad_norm': 719.6484985351562, 'learning_rate': 8e-06, 'epoch': 0.89}
{'loss': 37.5, 'grad_norm': 543.8240966796875, 'learning_rate': 8e-06, 'epoch': 0.89}
{'loss': 53.25, 'grad_norm': 1267.9139404296875, 'learning_rate': 8e-06, 'epoch': 0.9}
{'loss': 74.0, 'grad_norm': 1041.6893310546875, 'learning_rate': 8e-06, 'epoch': 0.91}
{'eval_loss': 46.28166580200195, 'eval_runtime': 9.902, 'eval_samples_per_second': 60.594, 'eval_steps_per_second': 3.838, 'epoch': 0.91}
{'loss': 31.875, 'grad_norm': 540.0499877929688, 'learning_rate': 8e-06, 'epoch': 0.91}
{'loss': 29.5, 'grad_norm': 735.5051879882812, 'learning_rate': 8e-06, 'epoch': 0.92}
{'loss': 24.375, 'grad_norm': 512.9950561523438, 'learning_rate': 8e-06, 'epoch': 0.93}
{'loss': 41.0, 'grad_norm': 958.8057250976562, 'learning_rate': 8e-06, 'epoch': 0.93}
{'eval_loss': 45.893333435058594, 'eval_runtime': 9.9136, 'eval_samples_per_second': 60.523, 'eval_steps_per_second': 3.833, 'epoch': 0.93}
{'loss': 25.875, 'grad_norm': 494.78009033203125, 'learning_rate': 8e-06, 'epoch': 0.94}
{'loss': 25.0, 'grad_norm': 508.3284606933594, 'learning_rate': 8e-06, 'epoch': 0.95}
{'loss': 24.0, 'grad_norm': 483.69866943359375, 'learning_rate': 8e-06, 'epoch': 0.95}
{'loss': 41.75, 'grad_norm': 605.3123779296875, 'learning_rate': 8e-06, 'epoch': 0.96}
{'eval_loss': 45.20833206176758, 'eval_runtime': 9.9133, 'eval_samples_per_second': 60.525, 'eval_steps_per_second': 3.833, 'epoch': 0.96}
{'loss': 33.75, 'grad_norm': 472.4231872558594, 'learning_rate': 8e-06, 'epoch': 0.97}
{'loss': 24.5, 'grad_norm': 533.3406372070312, 'learning_rate': 8e-06, 'epoch': 0.97}
{'loss': 28.75, 'grad_norm': 427.2243957519531, 'learning_rate': 8e-06, 'epoch': 0.98}
{'loss': 62.0, 'grad_norm': 662.604736328125, 'learning_rate': 8e-06, 'epoch': 0.99}
{'eval_loss': 44.663333892822266, 'eval_runtime': 9.9053, 'eval_samples_per_second': 60.574, 'eval_steps_per_second': 3.836, 'epoch': 0.99}
{'loss': 183.0, 'grad_norm': 1905.32958984375, 'learning_rate': 8e-06, 'epoch': 0.99}
{'loss': 22.125, 'grad_norm': 479.8912048339844, 'learning_rate': 8e-06, 'epoch': 1.0}
{'loss': 22.0, 'grad_norm': 347.0653076171875, 'learning_rate': 8e-06, 'epoch': 1.01}
{'loss': 55.25, 'grad_norm': 1353.0906982421875, 'learning_rate': 8e-06, 'epoch': 1.01}
{'eval_loss': 45.731666564941406, 'eval_runtime': 9.913, 'eval_samples_per_second': 60.527, 'eval_steps_per_second': 3.833, 'epoch': 1.01}
{'loss': 14.5625, 'grad_norm': 740.71826171875, 'learning_rate': 8e-06, 'epoch': 1.02}
{'loss': 13.0625, 'grad_norm': 295.7449035644531, 'learning_rate': 8e-06, 'epoch': 1.03}
{'loss': 35.0, 'grad_norm': 737.9833374023438, 'learning_rate': 8e-06, 'epoch': 1.03}
{'loss': 28.375, 'grad_norm': 584.7342529296875, 'learning_rate': 8e-06, 'epoch': 1.04}
{'eval_loss': 45.5533332824707, 'eval_runtime': 9.9078, 'eval_samples_per_second': 60.558, 'eval_steps_per_second': 3.835, 'epoch': 1.04}
{'loss': 74.0, 'grad_norm': 2052.0810546875, 'learning_rate': 8e-06, 'epoch': 1.05}
{'loss': 54.75, 'grad_norm': 980.9221801757812, 'learning_rate': 8e-06, 'epoch': 1.05}
{'loss': 36.25, 'grad_norm': 582.7664794921875, 'learning_rate': 8e-06, 'epoch': 1.06}
{'loss': 15.375, 'grad_norm': 281.6627502441406, 'learning_rate': 8e-06, 'epoch': 1.07}
{'eval_loss': 45.22666549682617, 'eval_runtime': 10.1365, 'eval_samples_per_second': 59.192, 'eval_steps_per_second': 3.749, 'epoch': 1.07}
{'loss': 28.375, 'grad_norm': 689.2493286132812, 'learning_rate': 8e-06, 'epoch': 1.07}
{'loss': 29.0, 'grad_norm': 466.1927490234375, 'learning_rate': 8e-06, 'epoch': 1.08}
{'loss': 20.875, 'grad_norm': 397.1120300292969, 'learning_rate': 8e-06, 'epoch': 1.09}
{'loss': 15.4375, 'grad_norm': 350.2940368652344, 'learning_rate': 8e-06, 'epoch': 1.09}
{'eval_loss': 44.56833267211914, 'eval_runtime': 9.9545, 'eval_samples_per_second': 60.274, 'eval_steps_per_second': 3.817, 'epoch': 1.09}
{'loss': 28.125, 'grad_norm': 536.8154907226562, 'learning_rate': 8e-06, 'epoch': 1.1}
{'loss': 22.0, 'grad_norm': 481.0686950683594, 'learning_rate': 8e-06, 'epoch': 1.11}
{'loss': 23.625, 'grad_norm': 390.0823059082031, 'learning_rate': 8e-06, 'epoch': 1.11}
{'loss': 25.25, 'grad_norm': 409.61444091796875, 'learning_rate': 8e-06, 'epoch': 1.12}
{'eval_loss': 44.246665954589844, 'eval_runtime': 10.1008, 'eval_samples_per_second': 59.401, 'eval_steps_per_second': 3.762, 'epoch': 1.12}
{'loss': 45.25, 'grad_norm': 754.155029296875, 'learning_rate': 8e-06, 'epoch': 1.13}
{'loss': 25.625, 'grad_norm': 583.121337890625, 'learning_rate': 8e-06, 'epoch': 1.13}
{'loss': 89.0, 'grad_norm': 1215.9111328125, 'learning_rate': 8e-06, 'epoch': 1.14}
{'loss': 48.5, 'grad_norm': 2233.065673828125, 'learning_rate': 8e-06, 'epoch': 1.15}
{'eval_loss': 45.33333206176758, 'eval_runtime': 10.1132, 'eval_samples_per_second': 59.328, 'eval_steps_per_second': 3.757, 'epoch': 1.15}
{'loss': 37.75, 'grad_norm': 699.7655639648438, 'learning_rate': 8e-06, 'epoch': 1.15}
{'loss': 51.25, 'grad_norm': 662.075927734375, 'learning_rate': 8e-06, 'epoch': 1.16}
{'loss': 26.25, 'grad_norm': 502.00439453125, 'learning_rate': 8e-06, 'epoch': 1.17}
{'loss': 14.8125, 'grad_norm': 370.27862548828125, 'learning_rate': 8e-06, 'epoch': 1.17}
{'eval_loss': 43.56666564941406, 'eval_runtime': 10.0859, 'eval_samples_per_second': 59.489, 'eval_steps_per_second': 3.768, 'epoch': 1.17}
{'loss': 18.25, 'grad_norm': 327.3307189941406, 'learning_rate': 8e-06, 'epoch': 1.18}
{'loss': 49.75, 'grad_norm': 760.1254272460938, 'learning_rate': 8e-06, 'epoch': 1.19}
{'loss': 40.5, 'grad_norm': 523.3978881835938, 'learning_rate': 8e-06, 'epoch': 1.19}
{'loss': 41.0, 'grad_norm': 661.8865356445312, 'learning_rate': 8e-06, 'epoch': 1.2}
{'eval_loss': 44.09000015258789, 'eval_runtime': 9.9203, 'eval_samples_per_second': 60.482, 'eval_steps_per_second': 3.831, 'epoch': 1.2}
{'loss': 27.125, 'grad_norm': 551.1517333984375, 'learning_rate': 8e-06, 'epoch': 1.21}
{'loss': 58.0, 'grad_norm': 997.2457885742188, 'learning_rate': 8e-06, 'epoch': 1.21}
{'loss': 57.0, 'grad_norm': 866.2943725585938, 'learning_rate': 8e-06, 'epoch': 1.22}
{'loss': 29.125, 'grad_norm': 625.1732177734375, 'learning_rate': 8e-06, 'epoch': 1.23}
{'eval_loss': 43.526668548583984, 'eval_runtime': 10.0684, 'eval_samples_per_second': 59.592, 'eval_steps_per_second': 3.774, 'epoch': 1.23}
{'loss': 28.875, 'grad_norm': 492.9136962890625, 'learning_rate': 8e-06, 'epoch': 1.23}
{'loss': 44.0, 'grad_norm': 1257.7257080078125, 'learning_rate': 8e-06, 'epoch': 1.24}
{'loss': 31.5, 'grad_norm': 762.7109375, 'learning_rate': 8e-06, 'epoch': 1.25}
{'loss': 15.6875, 'grad_norm': 406.75738525390625, 'learning_rate': 8e-06, 'epoch': 1.25}
{'eval_loss': 43.724998474121094, 'eval_runtime': 9.9263, 'eval_samples_per_second': 60.445, 'eval_steps_per_second': 3.828, 'epoch': 1.25}
{'loss': 35.5, 'grad_norm': 575.8656616210938, 'learning_rate': 8e-06, 'epoch': 1.26}
{'loss': 30.0, 'grad_norm': 637.089599609375, 'learning_rate': 8e-06, 'epoch': 1.27}
{'loss': 63.0, 'grad_norm': 713.9056396484375, 'learning_rate': 8e-06, 'epoch': 1.27}
{'loss': 32.5, 'grad_norm': 569.176025390625, 'learning_rate': 8e-06, 'epoch': 1.28}
{'eval_loss': 43.47666549682617, 'eval_runtime': 10.0898, 'eval_samples_per_second': 59.466, 'eval_steps_per_second': 3.766, 'epoch': 1.28}
{'loss': 77.0, 'grad_norm': 1649.7896728515625, 'learning_rate': 8e-06, 'epoch': 1.29}
{'loss': 24.75, 'grad_norm': 714.840087890625, 'learning_rate': 8e-06, 'epoch': 1.29}
{'loss': 48.25, 'grad_norm': 658.888671875, 'learning_rate': 8e-06, 'epoch': 1.3}
{'loss': 15.75, 'grad_norm': 588.48779296875, 'learning_rate': 8e-06, 'epoch': 1.31}
{'eval_loss': 43.391666412353516, 'eval_runtime': 9.9077, 'eval_samples_per_second': 60.559, 'eval_steps_per_second': 3.835, 'epoch': 1.31}
{'loss': 20.0, 'grad_norm': 453.17095947265625, 'learning_rate': 8e-06, 'epoch': 1.31}
{'loss': 30.125, 'grad_norm': 508.571533203125, 'learning_rate': 8e-06, 'epoch': 1.32}
{'loss': 39.75, 'grad_norm': 632.2625732421875, 'learning_rate': 8e-06, 'epoch': 1.33}
{'loss': 45.5, 'grad_norm': 824.5717163085938, 'learning_rate': 8e-06, 'epoch': 1.33}
{'eval_loss': 42.858333587646484, 'eval_runtime': 9.9257, 'eval_samples_per_second': 60.449, 'eval_steps_per_second': 3.828, 'epoch': 1.33}
{'loss': 22.625, 'grad_norm': 418.8602600097656, 'learning_rate': 8e-06, 'epoch': 1.34}
{'loss': 28.125, 'grad_norm': 647.5723876953125, 'learning_rate': 8e-06, 'epoch': 1.35}
{'loss': 22.875, 'grad_norm': 542.2384643554688, 'learning_rate': 8e-06, 'epoch': 1.35}
{'loss': 44.25, 'grad_norm': 665.0690307617188, 'learning_rate': 8e-06, 'epoch': 1.36}
{'eval_loss': 42.80666732788086, 'eval_runtime': 10.1066, 'eval_samples_per_second': 59.367, 'eval_steps_per_second': 3.76, 'epoch': 1.36}
{'loss': 38.75, 'grad_norm': 527.682861328125, 'learning_rate': 8e-06, 'epoch': 1.37}
{'loss': 67.0, 'grad_norm': 1111.892822265625, 'learning_rate': 8e-06, 'epoch': 1.37}
{'loss': 27.25, 'grad_norm': 341.7921447753906, 'learning_rate': 8e-06, 'epoch': 1.38}
{'loss': 29.5, 'grad_norm': 358.5734558105469, 'learning_rate': 8e-06, 'epoch': 1.39}
{'eval_loss': 42.09000015258789, 'eval_runtime': 9.9195, 'eval_samples_per_second': 60.487, 'eval_steps_per_second': 3.831, 'epoch': 1.39}
{'loss': 36.75, 'grad_norm': 523.5665893554688, 'learning_rate': 8e-06, 'epoch': 1.39}
{'loss': 32.75, 'grad_norm': 565.5841674804688, 'learning_rate': 8e-06, 'epoch': 1.4}
{'loss': 23.75, 'grad_norm': 463.4585266113281, 'learning_rate': 8e-06, 'epoch': 1.41}
{'loss': 45.25, 'grad_norm': 585.2896728515625, 'learning_rate': 8e-06, 'epoch': 1.41}
{'eval_loss': 42.43166732788086, 'eval_runtime': 10.0921, 'eval_samples_per_second': 59.452, 'eval_steps_per_second': 3.765, 'epoch': 1.41}
{'loss': 27.75, 'grad_norm': 579.4701538085938, 'learning_rate': 8e-06, 'epoch': 1.42}
{'loss': 29.5, 'grad_norm': 539.7914428710938, 'learning_rate': 8e-06, 'epoch': 1.43}
{'loss': 25.875, 'grad_norm': 561.966552734375, 'learning_rate': 8e-06, 'epoch': 1.43}
{'loss': 62.5, 'grad_norm': 870.224609375, 'learning_rate': 8e-06, 'epoch': 1.44}
{'eval_loss': 41.95000076293945, 'eval_runtime': 9.9169, 'eval_samples_per_second': 60.503, 'eval_steps_per_second': 3.832, 'epoch': 1.44}
{'loss': 34.75, 'grad_norm': 698.2300415039062, 'learning_rate': 8e-06, 'epoch': 1.45}
{'loss': 41.75, 'grad_norm': 1027.5205078125, 'learning_rate': 8e-06, 'epoch': 1.45}
{'loss': 58.25, 'grad_norm': 1733.5338134765625, 'learning_rate': 8e-06, 'epoch': 1.46}
{'loss': 25.75, 'grad_norm': 529.229248046875, 'learning_rate': 8e-06, 'epoch': 1.47}
{'eval_loss': 41.94499969482422, 'eval_runtime': 10.0785, 'eval_samples_per_second': 59.533, 'eval_steps_per_second': 3.77, 'epoch': 1.47}
{'loss': 58.0, 'grad_norm': 1076.5008544921875, 'learning_rate': 8e-06, 'epoch': 1.47}
{'loss': 41.5, 'grad_norm': 946.3248291015625, 'learning_rate': 8e-06, 'epoch': 1.48}
{'loss': 14.8125, 'grad_norm': 450.47308349609375, 'learning_rate': 8e-06, 'epoch': 1.49}
{'loss': 23.5, 'grad_norm': 494.0532531738281, 'learning_rate': 8e-06, 'epoch': 1.49}
{'eval_loss': 42.70833206176758, 'eval_runtime': 10.1125, 'eval_samples_per_second': 59.333, 'eval_steps_per_second': 3.758, 'epoch': 1.49}
{'loss': 30.75, 'grad_norm': 639.2837524414062, 'learning_rate': 8e-06, 'epoch': 1.5}
{'loss': 25.0, 'grad_norm': 480.48638916015625, 'learning_rate': 8e-06, 'epoch': 1.51}
{'loss': 15.8125, 'grad_norm': 529.5014038085938, 'learning_rate': 8e-06, 'epoch': 1.51}
{'loss': 28.0, 'grad_norm': 461.96307373046875, 'learning_rate': 8e-06, 'epoch': 1.52}
{'eval_loss': 42.0, 'eval_runtime': 9.9121, 'eval_samples_per_second': 60.532, 'eval_steps_per_second': 3.834, 'epoch': 1.52}
{'loss': 18.125, 'grad_norm': 380.4690856933594, 'learning_rate': 8e-06, 'epoch': 1.53}
{'loss': 32.5, 'grad_norm': 555.973876953125, 'learning_rate': 8e-06, 'epoch': 1.53}
{'loss': 22.25, 'grad_norm': 418.0882568359375, 'learning_rate': 8e-06, 'epoch': 1.54}
{'loss': 26.875, 'grad_norm': 783.1881713867188, 'learning_rate': 8e-06, 'epoch': 1.55}
{'eval_loss': 42.165000915527344, 'eval_runtime': 9.9209, 'eval_samples_per_second': 60.478, 'eval_steps_per_second': 3.83, 'epoch': 1.55}
{'loss': 36.0, 'grad_norm': 486.3771667480469, 'learning_rate': 8e-06, 'epoch': 1.55}
{'loss': 28.375, 'grad_norm': 631.2808227539062, 'learning_rate': 8e-06, 'epoch': 1.56}
{'loss': 23.25, 'grad_norm': 471.6466979980469, 'learning_rate': 8e-06, 'epoch': 1.57}
{'loss': 21.125, 'grad_norm': 447.32623291015625, 'learning_rate': 8e-06, 'epoch': 1.57}
{'eval_loss': 41.744998931884766, 'eval_runtime': 9.9342, 'eval_samples_per_second': 60.398, 'eval_steps_per_second': 3.825, 'epoch': 1.57}
{'loss': 16.125, 'grad_norm': 384.1124267578125, 'learning_rate': 8e-06, 'epoch': 1.58}
{'loss': 46.75, 'grad_norm': 917.4016723632812, 'learning_rate': 8e-06, 'epoch': 1.59}
{'loss': 22.875, 'grad_norm': 498.5153503417969, 'learning_rate': 8e-06, 'epoch': 1.59}
{'loss': 18.625, 'grad_norm': 383.8331298828125, 'learning_rate': 8e-06, 'epoch': 1.6}
{'eval_loss': 42.4183349609375, 'eval_runtime': 10.1565, 'eval_samples_per_second': 59.076, 'eval_steps_per_second': 3.741, 'epoch': 1.6}
{'loss': 46.0, 'grad_norm': 1079.0074462890625, 'learning_rate': 8e-06, 'epoch': 1.61}
{'loss': 28.625, 'grad_norm': 907.87890625, 'learning_rate': 8e-06, 'epoch': 1.61}
{'loss': 56.75, 'grad_norm': 818.578369140625, 'learning_rate': 8e-06, 'epoch': 1.62}
{'loss': 49.25, 'grad_norm': 1060.4222412109375, 'learning_rate': 8e-06, 'epoch': 1.63}
{'eval_loss': 41.974998474121094, 'eval_runtime': 9.9306, 'eval_samples_per_second': 60.419, 'eval_steps_per_second': 3.827, 'epoch': 1.63}
{'loss': 44.5, 'grad_norm': 896.1473388671875, 'learning_rate': 8e-06, 'epoch': 1.63}
{'loss': 52.5, 'grad_norm': 922.6982421875, 'learning_rate': 8e-06, 'epoch': 1.64}
{'loss': 19.875, 'grad_norm': 454.3968505859375, 'learning_rate': 8e-06, 'epoch': 1.65}
{'loss': 62.0, 'grad_norm': 1167.5714111328125, 'learning_rate': 8e-06, 'epoch': 1.65}
{'eval_loss': 41.474998474121094, 'eval_runtime': 10.1156, 'eval_samples_per_second': 59.314, 'eval_steps_per_second': 3.757, 'epoch': 1.65}
{'loss': 32.0, 'grad_norm': 576.5531616210938, 'learning_rate': 8e-06, 'epoch': 1.66}
{'loss': 30.625, 'grad_norm': 424.8453063964844, 'learning_rate': 8e-06, 'epoch': 1.67}
{'loss': 25.125, 'grad_norm': 586.62646484375, 'learning_rate': 8e-06, 'epoch': 1.67}
{'loss': 28.25, 'grad_norm': 590.21435546875, 'learning_rate': 8e-06, 'epoch': 1.68}
{'eval_loss': 40.8125, 'eval_runtime': 9.8892, 'eval_samples_per_second': 60.672, 'eval_steps_per_second': 3.843, 'epoch': 1.68}
{'loss': 25.625, 'grad_norm': 405.1659240722656, 'learning_rate': 8e-06, 'epoch': 1.69}
{'loss': 24.875, 'grad_norm': 510.9698486328125, 'learning_rate': 8e-06, 'epoch': 1.69}
{'loss': 24.125, 'grad_norm': 633.0056762695312, 'learning_rate': 8e-06, 'epoch': 1.7}
{'loss': 16.25, 'grad_norm': 347.87261962890625, 'learning_rate': 8e-06, 'epoch': 1.71}
{'eval_loss': 40.56999969482422, 'eval_runtime': 10.1001, 'eval_samples_per_second': 59.405, 'eval_steps_per_second': 3.762, 'epoch': 1.71}
{'loss': 21.25, 'grad_norm': 353.23992919921875, 'learning_rate': 8e-06, 'epoch': 1.71}
{'loss': 32.0, 'grad_norm': 502.0780029296875, 'learning_rate': 8e-06, 'epoch': 1.72}
{'loss': 15.8125, 'grad_norm': 286.9412536621094, 'learning_rate': 8e-06, 'epoch': 1.73}
{'loss': 31.125, 'grad_norm': 471.6520080566406, 'learning_rate': 8e-06, 'epoch': 1.73}
{'eval_loss': 40.86916732788086, 'eval_runtime': 9.8869, 'eval_samples_per_second': 60.686, 'eval_steps_per_second': 3.843, 'epoch': 1.73}
{'loss': 31.375, 'grad_norm': 518.0714111328125, 'learning_rate': 8e-06, 'epoch': 1.74}
{'loss': 81.5, 'grad_norm': 4756.32275390625, 'learning_rate': 8e-06, 'epoch': 1.75}
{'loss': 16.125, 'grad_norm': 356.7523193359375, 'learning_rate': 8e-06, 'epoch': 1.75}
{'loss': 22.75, 'grad_norm': 345.4314880371094, 'learning_rate': 8e-06, 'epoch': 1.76}
{'eval_loss': 40.55666732788086, 'eval_runtime': 9.915, 'eval_samples_per_second': 60.514, 'eval_steps_per_second': 3.833, 'epoch': 1.76}
{'loss': 22.125, 'grad_norm': 375.454833984375, 'learning_rate': 8e-06, 'epoch': 1.77}
{'loss': 42.0, 'grad_norm': 652.0215454101562, 'learning_rate': 8e-06, 'epoch': 1.77}
{'loss': 43.5, 'grad_norm': 688.601806640625, 'learning_rate': 8e-06, 'epoch': 1.78}
{'loss': 19.875, 'grad_norm': 430.4117126464844, 'learning_rate': 8e-06, 'epoch': 1.79}
{'eval_loss': 40.67333221435547, 'eval_runtime': 9.9261, 'eval_samples_per_second': 60.447, 'eval_steps_per_second': 3.828, 'epoch': 1.79}
{'loss': 21.625, 'grad_norm': 518.3961181640625, 'learning_rate': 8e-06, 'epoch': 1.79}
{'loss': 15.5625, 'grad_norm': 420.8150634765625, 'learning_rate': 8e-06, 'epoch': 1.8}
{'loss': 22.875, 'grad_norm': 485.0090637207031, 'learning_rate': 8e-06, 'epoch': 1.81}
{'loss': 19.5, 'grad_norm': 371.49176025390625, 'learning_rate': 8e-06, 'epoch': 1.81}
{'eval_loss': 40.345001220703125, 'eval_runtime': 10.1337, 'eval_samples_per_second': 59.208, 'eval_steps_per_second': 3.75, 'epoch': 1.81}
{'loss': 43.5, 'grad_norm': 540.4852294921875, 'learning_rate': 8e-06, 'epoch': 1.82}
{'loss': 45.25, 'grad_norm': 587.0624389648438, 'learning_rate': 8e-06, 'epoch': 1.83}
{'loss': 24.625, 'grad_norm': 462.47998046875, 'learning_rate': 8e-06, 'epoch': 1.83}
{'loss': 33.5, 'grad_norm': 587.6112670898438, 'learning_rate': 8e-06, 'epoch': 1.84}
{'eval_loss': 40.21083450317383, 'eval_runtime': 9.9313, 'eval_samples_per_second': 60.415, 'eval_steps_per_second': 3.826, 'epoch': 1.84}
{'loss': 19.75, 'grad_norm': 382.31622314453125, 'learning_rate': 8e-06, 'epoch': 1.85}
{'loss': 22.5, 'grad_norm': 457.2533874511719, 'learning_rate': 8e-06, 'epoch': 1.85}
{'loss': 60.5, 'grad_norm': 888.7138061523438, 'learning_rate': 8e-06, 'epoch': 1.86}
{'loss': 16.0, 'grad_norm': 583.305908203125, 'learning_rate': 8e-06, 'epoch': 1.87}
{'eval_loss': 39.790000915527344, 'eval_runtime': 9.9187, 'eval_samples_per_second': 60.492, 'eval_steps_per_second': 3.831, 'epoch': 1.87}
{'loss': 51.25, 'grad_norm': 865.2531127929688, 'learning_rate': 8e-06, 'epoch': 1.87}
{'loss': 17.125, 'grad_norm': 340.26287841796875, 'learning_rate': 8e-06, 'epoch': 1.88}
{'loss': 31.25, 'grad_norm': 514.014404296875, 'learning_rate': 8e-06, 'epoch': 1.89}
{'loss': 25.75, 'grad_norm': 467.27386474609375, 'learning_rate': 8e-06, 'epoch': 1.89}
{'eval_loss': 39.7599983215332, 'eval_runtime': 10.1506, 'eval_samples_per_second': 59.11, 'eval_steps_per_second': 3.744, 'epoch': 1.89}
{'loss': 30.5, 'grad_norm': 517.0917358398438, 'learning_rate': 8e-06, 'epoch': 1.9}
{'loss': 22.0, 'grad_norm': 368.6695556640625, 'learning_rate': 8e-06, 'epoch': 1.91}
{'loss': 24.125, 'grad_norm': 546.6204223632812, 'learning_rate': 8e-06, 'epoch': 1.91}
{'loss': 45.5, 'grad_norm': 1322.6490478515625, 'learning_rate': 8e-06, 'epoch': 1.92}
{'eval_loss': 39.61000061035156, 'eval_runtime': 10.0768, 'eval_samples_per_second': 59.543, 'eval_steps_per_second': 3.771, 'epoch': 1.92}
{'loss': 25.875, 'grad_norm': 382.2994079589844, 'learning_rate': 8e-06, 'epoch': 1.93}
{'loss': 21.125, 'grad_norm': 417.6145935058594, 'learning_rate': 8e-06, 'epoch': 1.93}
{'loss': 28.375, 'grad_norm': 530.2188110351562, 'learning_rate': 8e-06, 'epoch': 1.94}
{'loss': 21.25, 'grad_norm': 401.74969482421875, 'learning_rate': 8e-06, 'epoch': 1.95}
{'eval_loss': 39.26166534423828, 'eval_runtime': 9.9128, 'eval_samples_per_second': 60.528, 'eval_steps_per_second': 3.833, 'epoch': 1.95}
{'loss': 18.375, 'grad_norm': 398.7279052734375, 'learning_rate': 8e-06, 'epoch': 1.95}
{'loss': 23.0, 'grad_norm': 485.920654296875, 'learning_rate': 8e-06, 'epoch': 1.96}
{'loss': 20.125, 'grad_norm': 354.4771423339844, 'learning_rate': 8e-06, 'epoch': 1.97}
{'loss': 30.875, 'grad_norm': 424.4510192871094, 'learning_rate': 8e-06, 'epoch': 1.97}
{'eval_loss': 39.057498931884766, 'eval_runtime': 10.1, 'eval_samples_per_second': 59.406, 'eval_steps_per_second': 3.762, 'epoch': 1.97}
{'loss': 39.25, 'grad_norm': 504.2866516113281, 'learning_rate': 8e-06, 'epoch': 1.98}
{'loss': 37.0, 'grad_norm': 543.530029296875, 'learning_rate': 8e-06, 'epoch': 1.99}
{'loss': 45.5, 'grad_norm': 628.2706909179688, 'learning_rate': 8e-06, 'epoch': 1.99}
{'loss': 87.0, 'grad_norm': 2545.672607421875, 'learning_rate': 8e-06, 'epoch': 2.0}
{'eval_loss': 39.5283317565918, 'eval_runtime': 9.9093, 'eval_samples_per_second': 60.549, 'eval_steps_per_second': 3.835, 'epoch': 2.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [47:32<00:00,  5.71s/i/home/ubuntu/hpml-final/BitDistiller-Fork/train/venv/lib/python3.10/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  partition = torch.load(path, map_location=map_location)
{'train_runtime': 2887.2122, 'train_samples_per_second': 1.663, 'train_steps_per_second': 0.104, 'train_loss': 59.305625, 'epoch': 2.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [48:07<00:00,  9.62s/it]
[rank0]:[W1124 06:52:58.061014715 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[2024-11-24 06:53:02,530] [INFO] [launch.py:351:main] Process 14492 exits successfully.

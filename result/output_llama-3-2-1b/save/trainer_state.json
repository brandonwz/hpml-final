{
  "best_metric": 214.3866729736328,
  "best_model_checkpoint": "save/checkpoint-296",
  "epoch": 2.0,
  "eval_steps": 4,
  "global_step": 300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 89949.890625,
      "learning_rate": 8e-06,
      "loss": 2064.0,
      "step": 1
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 44560.85546875,
      "learning_rate": 8e-06,
      "loss": 1432.0,
      "step": 2
    },
    {
      "epoch": 0.02,
      "grad_norm": 29831.84375,
      "learning_rate": 8e-06,
      "loss": 1624.0,
      "step": 3
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 20308.662109375,
      "learning_rate": 8e-06,
      "loss": 1032.0,
      "step": 4
    },
    {
      "epoch": 0.02666666666666667,
      "eval_loss": 1060.21337890625,
      "eval_runtime": 32.4289,
      "eval_samples_per_second": 18.502,
      "eval_steps_per_second": 1.172,
      "step": 4
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 22240.201171875,
      "learning_rate": 8e-06,
      "loss": 1504.0,
      "step": 5
    },
    {
      "epoch": 0.04,
      "grad_norm": 15494.8486328125,
      "learning_rate": 8e-06,
      "loss": 660.0,
      "step": 6
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 10426.8095703125,
      "learning_rate": 8e-06,
      "loss": 498.0,
      "step": 7
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 19679.23046875,
      "learning_rate": 8e-06,
      "loss": 920.0,
      "step": 8
    },
    {
      "epoch": 0.05333333333333334,
      "eval_loss": 739.7333374023438,
      "eval_runtime": 31.6362,
      "eval_samples_per_second": 18.966,
      "eval_steps_per_second": 1.201,
      "step": 8
    },
    {
      "epoch": 0.06,
      "grad_norm": 17024.7109375,
      "learning_rate": 8e-06,
      "loss": 564.0,
      "step": 9
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 17491.744140625,
      "learning_rate": 8e-06,
      "loss": 832.0,
      "step": 10
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 38381.1484375,
      "learning_rate": 8e-06,
      "loss": 632.0,
      "step": 11
    },
    {
      "epoch": 0.08,
      "grad_norm": 7494.22119140625,
      "learning_rate": 8e-06,
      "loss": 548.0,
      "step": 12
    },
    {
      "epoch": 0.08,
      "eval_loss": 596.5066528320312,
      "eval_runtime": 32.1933,
      "eval_samples_per_second": 18.637,
      "eval_steps_per_second": 1.18,
      "step": 12
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 6059.35791015625,
      "learning_rate": 8e-06,
      "loss": 474.0,
      "step": 13
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 10780.681640625,
      "learning_rate": 8e-06,
      "loss": 486.0,
      "step": 14
    },
    {
      "epoch": 0.1,
      "grad_norm": 5865.2412109375,
      "learning_rate": 8e-06,
      "loss": 418.0,
      "step": 15
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 10684.603515625,
      "learning_rate": 8e-06,
      "loss": 644.0,
      "step": 16
    },
    {
      "epoch": 0.10666666666666667,
      "eval_loss": 510.9333190917969,
      "eval_runtime": 31.9991,
      "eval_samples_per_second": 18.751,
      "eval_steps_per_second": 1.188,
      "step": 16
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 7172.853515625,
      "learning_rate": 8e-06,
      "loss": 436.0,
      "step": 17
    },
    {
      "epoch": 0.12,
      "grad_norm": 4746.52587890625,
      "learning_rate": 8e-06,
      "loss": 456.0,
      "step": 18
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 5860.9873046875,
      "learning_rate": 8e-06,
      "loss": 442.0,
      "step": 19
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 4214.880859375,
      "learning_rate": 8e-06,
      "loss": 362.0,
      "step": 20
    },
    {
      "epoch": 0.13333333333333333,
      "eval_loss": 452.0,
      "eval_runtime": 32.1469,
      "eval_samples_per_second": 18.664,
      "eval_steps_per_second": 1.182,
      "step": 20
    },
    {
      "epoch": 0.14,
      "grad_norm": 3892.499755859375,
      "learning_rate": 8e-06,
      "loss": 296.0,
      "step": 21
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 5753.654296875,
      "learning_rate": 8e-06,
      "loss": 552.0,
      "step": 22
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 4878.28759765625,
      "learning_rate": 8e-06,
      "loss": 400.0,
      "step": 23
    },
    {
      "epoch": 0.16,
      "grad_norm": 4511.37353515625,
      "learning_rate": 8e-06,
      "loss": 402.0,
      "step": 24
    },
    {
      "epoch": 0.16,
      "eval_loss": 412.1866760253906,
      "eval_runtime": 31.8236,
      "eval_samples_per_second": 18.854,
      "eval_steps_per_second": 1.194,
      "step": 24
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 5274.05126953125,
      "learning_rate": 8e-06,
      "loss": 368.0,
      "step": 25
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 4644.529296875,
      "learning_rate": 8e-06,
      "loss": 288.0,
      "step": 26
    },
    {
      "epoch": 0.18,
      "grad_norm": 4097.53369140625,
      "learning_rate": 8e-06,
      "loss": 346.0,
      "step": 27
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 3597.6689453125,
      "learning_rate": 8e-06,
      "loss": 412.0,
      "step": 28
    },
    {
      "epoch": 0.18666666666666668,
      "eval_loss": 383.2266540527344,
      "eval_runtime": 32.0237,
      "eval_samples_per_second": 18.736,
      "eval_steps_per_second": 1.187,
      "step": 28
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 33933.2890625,
      "learning_rate": 8e-06,
      "loss": 520.0,
      "step": 29
    },
    {
      "epoch": 0.2,
      "grad_norm": 52219.15234375,
      "learning_rate": 8e-06,
      "loss": 516.0,
      "step": 30
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 3340.4912109375,
      "learning_rate": 8e-06,
      "loss": 330.0,
      "step": 31
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 2835.834228515625,
      "learning_rate": 8e-06,
      "loss": 255.0,
      "step": 32
    },
    {
      "epoch": 0.21333333333333335,
      "eval_loss": 369.97332763671875,
      "eval_runtime": 31.9806,
      "eval_samples_per_second": 18.761,
      "eval_steps_per_second": 1.188,
      "step": 32
    },
    {
      "epoch": 0.22,
      "grad_norm": 5005.69287109375,
      "learning_rate": 8e-06,
      "loss": 446.0,
      "step": 33
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 3784.859375,
      "learning_rate": 8e-06,
      "loss": 306.0,
      "step": 34
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 2753.728271484375,
      "learning_rate": 8e-06,
      "loss": 320.0,
      "step": 35
    },
    {
      "epoch": 0.24,
      "grad_norm": 5076.19775390625,
      "learning_rate": 8e-06,
      "loss": 358.0,
      "step": 36
    },
    {
      "epoch": 0.24,
      "eval_loss": 353.0133361816406,
      "eval_runtime": 32.0192,
      "eval_samples_per_second": 18.739,
      "eval_steps_per_second": 1.187,
      "step": 36
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 3746.4365234375,
      "learning_rate": 8e-06,
      "loss": 304.0,
      "step": 37
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 3780.13818359375,
      "learning_rate": 8e-06,
      "loss": 462.0,
      "step": 38
    },
    {
      "epoch": 0.26,
      "grad_norm": 6162.32666015625,
      "learning_rate": 8e-06,
      "loss": 378.0,
      "step": 39
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 4977.2060546875,
      "learning_rate": 8e-06,
      "loss": 306.0,
      "step": 40
    },
    {
      "epoch": 0.26666666666666666,
      "eval_loss": 346.55999755859375,
      "eval_runtime": 31.9142,
      "eval_samples_per_second": 18.8,
      "eval_steps_per_second": 1.191,
      "step": 40
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 6961.6708984375,
      "learning_rate": 8e-06,
      "loss": 424.0,
      "step": 41
    },
    {
      "epoch": 0.28,
      "grad_norm": 2606.074462890625,
      "learning_rate": 8e-06,
      "loss": 208.0,
      "step": 42
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 2469.957275390625,
      "learning_rate": 8e-06,
      "loss": 270.0,
      "step": 43
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 3760.159423828125,
      "learning_rate": 8e-06,
      "loss": 380.0,
      "step": 44
    },
    {
      "epoch": 0.29333333333333333,
      "eval_loss": 336.3066711425781,
      "eval_runtime": 31.9297,
      "eval_samples_per_second": 18.791,
      "eval_steps_per_second": 1.19,
      "step": 44
    },
    {
      "epoch": 0.3,
      "grad_norm": 3113.09130859375,
      "learning_rate": 8e-06,
      "loss": 404.0,
      "step": 45
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 12036.275390625,
      "learning_rate": 8e-06,
      "loss": 396.0,
      "step": 46
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 3066.453857421875,
      "learning_rate": 8e-06,
      "loss": 348.0,
      "step": 47
    },
    {
      "epoch": 0.32,
      "grad_norm": 2505.77001953125,
      "learning_rate": 8e-06,
      "loss": 350.0,
      "step": 48
    },
    {
      "epoch": 0.32,
      "eval_loss": 327.67999267578125,
      "eval_runtime": 32.1077,
      "eval_samples_per_second": 18.687,
      "eval_steps_per_second": 1.184,
      "step": 48
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 86790.0390625,
      "learning_rate": 8e-06,
      "loss": 270.0,
      "step": 49
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1515.45556640625,
      "learning_rate": 8e-06,
      "loss": 147.0,
      "step": 50
    },
    {
      "epoch": 0.34,
      "grad_norm": 65217.9609375,
      "learning_rate": 8e-06,
      "loss": 476.0,
      "step": 51
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 8575.0224609375,
      "learning_rate": 8e-06,
      "loss": 318.0,
      "step": 52
    },
    {
      "epoch": 0.3466666666666667,
      "eval_loss": 329.6933288574219,
      "eval_runtime": 31.98,
      "eval_samples_per_second": 18.762,
      "eval_steps_per_second": 1.188,
      "step": 52
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 16578.6484375,
      "learning_rate": 8e-06,
      "loss": 588.0,
      "step": 53
    },
    {
      "epoch": 0.36,
      "grad_norm": 4032.284423828125,
      "learning_rate": 8e-06,
      "loss": 262.0,
      "step": 54
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 3688.390869140625,
      "learning_rate": 8e-06,
      "loss": 430.0,
      "step": 55
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 2822.803466796875,
      "learning_rate": 8e-06,
      "loss": 298.0,
      "step": 56
    },
    {
      "epoch": 0.37333333333333335,
      "eval_loss": 315.5733337402344,
      "eval_runtime": 31.9391,
      "eval_samples_per_second": 18.786,
      "eval_steps_per_second": 1.19,
      "step": 56
    },
    {
      "epoch": 0.38,
      "grad_norm": 2266.776611328125,
      "learning_rate": 8e-06,
      "loss": 536.0,
      "step": 57
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 2017.859375,
      "learning_rate": 8e-06,
      "loss": 234.0,
      "step": 58
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 1764.14990234375,
      "learning_rate": 8e-06,
      "loss": 241.0,
      "step": 59
    },
    {
      "epoch": 0.4,
      "grad_norm": 4145.65869140625,
      "learning_rate": 8e-06,
      "loss": 404.0,
      "step": 60
    },
    {
      "epoch": 0.4,
      "eval_loss": 308.2266540527344,
      "eval_runtime": 31.9078,
      "eval_samples_per_second": 18.804,
      "eval_steps_per_second": 1.191,
      "step": 60
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 3434.65234375,
      "learning_rate": 8e-06,
      "loss": 346.0,
      "step": 61
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 2944.703369140625,
      "learning_rate": 8e-06,
      "loss": 468.0,
      "step": 62
    },
    {
      "epoch": 0.42,
      "grad_norm": 2770.421142578125,
      "learning_rate": 8e-06,
      "loss": 358.0,
      "step": 63
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 2922.893310546875,
      "learning_rate": 8e-06,
      "loss": 274.0,
      "step": 64
    },
    {
      "epoch": 0.4266666666666667,
      "eval_loss": 301.82666015625,
      "eval_runtime": 31.9527,
      "eval_samples_per_second": 18.778,
      "eval_steps_per_second": 1.189,
      "step": 64
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 2098.89599609375,
      "learning_rate": 8e-06,
      "loss": 148.0,
      "step": 65
    },
    {
      "epoch": 0.44,
      "grad_norm": 35543.05078125,
      "learning_rate": 8e-06,
      "loss": 272.0,
      "step": 66
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 3034.348388671875,
      "learning_rate": 8e-06,
      "loss": 322.0,
      "step": 67
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 3237.279052734375,
      "learning_rate": 8e-06,
      "loss": 346.0,
      "step": 68
    },
    {
      "epoch": 0.4533333333333333,
      "eval_loss": 298.0400085449219,
      "eval_runtime": 31.9052,
      "eval_samples_per_second": 18.806,
      "eval_steps_per_second": 1.191,
      "step": 68
    },
    {
      "epoch": 0.46,
      "grad_norm": 3835.883544921875,
      "learning_rate": 8e-06,
      "loss": 296.0,
      "step": 69
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 2370.7080078125,
      "learning_rate": 8e-06,
      "loss": 225.0,
      "step": 70
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 2168.486572265625,
      "learning_rate": 8e-06,
      "loss": 308.0,
      "step": 71
    },
    {
      "epoch": 0.48,
      "grad_norm": 1694.1556396484375,
      "learning_rate": 8e-06,
      "loss": 197.0,
      "step": 72
    },
    {
      "epoch": 0.48,
      "eval_loss": 293.7200012207031,
      "eval_runtime": 31.9927,
      "eval_samples_per_second": 18.754,
      "eval_steps_per_second": 1.188,
      "step": 72
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 8832.2841796875,
      "learning_rate": 8e-06,
      "loss": 182.0,
      "step": 73
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 4866.208984375,
      "learning_rate": 8e-06,
      "loss": 236.0,
      "step": 74
    },
    {
      "epoch": 0.5,
      "grad_norm": 2837.202392578125,
      "learning_rate": 8e-06,
      "loss": 266.0,
      "step": 75
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 5619.4482421875,
      "learning_rate": 8e-06,
      "loss": 298.0,
      "step": 76
    },
    {
      "epoch": 0.5066666666666667,
      "eval_loss": 290.9200134277344,
      "eval_runtime": 31.9697,
      "eval_samples_per_second": 18.768,
      "eval_steps_per_second": 1.189,
      "step": 76
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 3021.235595703125,
      "learning_rate": 8e-06,
      "loss": 260.0,
      "step": 77
    },
    {
      "epoch": 0.52,
      "grad_norm": 3570.1083984375,
      "learning_rate": 8e-06,
      "loss": 278.0,
      "step": 78
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 2240.2548828125,
      "learning_rate": 8e-06,
      "loss": 246.0,
      "step": 79
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 12610.744140625,
      "learning_rate": 8e-06,
      "loss": 298.0,
      "step": 80
    },
    {
      "epoch": 0.5333333333333333,
      "eval_loss": 290.2266540527344,
      "eval_runtime": 31.9408,
      "eval_samples_per_second": 18.785,
      "eval_steps_per_second": 1.19,
      "step": 80
    },
    {
      "epoch": 0.54,
      "grad_norm": 4132.1181640625,
      "learning_rate": 8e-06,
      "loss": 326.0,
      "step": 81
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 5065.22998046875,
      "learning_rate": 8e-06,
      "loss": 270.0,
      "step": 82
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 2113.701171875,
      "learning_rate": 8e-06,
      "loss": 224.0,
      "step": 83
    },
    {
      "epoch": 0.56,
      "grad_norm": 1480.390625,
      "learning_rate": 8e-06,
      "loss": 181.0,
      "step": 84
    },
    {
      "epoch": 0.56,
      "eval_loss": 284.0533447265625,
      "eval_runtime": 31.8666,
      "eval_samples_per_second": 18.828,
      "eval_steps_per_second": 1.192,
      "step": 84
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 6685.3857421875,
      "learning_rate": 8e-06,
      "loss": 222.0,
      "step": 85
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 1862.138427734375,
      "learning_rate": 8e-06,
      "loss": 238.0,
      "step": 86
    },
    {
      "epoch": 0.58,
      "grad_norm": 3165.844482421875,
      "learning_rate": 8e-06,
      "loss": 334.0,
      "step": 87
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 3033.8125,
      "learning_rate": 8e-06,
      "loss": 234.0,
      "step": 88
    },
    {
      "epoch": 0.5866666666666667,
      "eval_loss": 284.0400085449219,
      "eval_runtime": 31.7438,
      "eval_samples_per_second": 18.901,
      "eval_steps_per_second": 1.197,
      "step": 88
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 2981.123046875,
      "learning_rate": 8e-06,
      "loss": 330.0,
      "step": 89
    },
    {
      "epoch": 0.6,
      "grad_norm": 1989.9962158203125,
      "learning_rate": 8e-06,
      "loss": 245.0,
      "step": 90
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 2681.495361328125,
      "learning_rate": 8e-06,
      "loss": 158.0,
      "step": 91
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 2033.550048828125,
      "learning_rate": 8e-06,
      "loss": 208.0,
      "step": 92
    },
    {
      "epoch": 0.6133333333333333,
      "eval_loss": 277.5199890136719,
      "eval_runtime": 32.0382,
      "eval_samples_per_second": 18.728,
      "eval_steps_per_second": 1.186,
      "step": 92
    },
    {
      "epoch": 0.62,
      "grad_norm": 29639.810546875,
      "learning_rate": 8e-06,
      "loss": 235.0,
      "step": 93
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 1787.7860107421875,
      "learning_rate": 8e-06,
      "loss": 189.0,
      "step": 94
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 1431.7733154296875,
      "learning_rate": 8e-06,
      "loss": 166.0,
      "step": 95
    },
    {
      "epoch": 0.64,
      "grad_norm": 3233.198486328125,
      "learning_rate": 8e-06,
      "loss": 286.0,
      "step": 96
    },
    {
      "epoch": 0.64,
      "eval_loss": 274.7733459472656,
      "eval_runtime": 32.1659,
      "eval_samples_per_second": 18.653,
      "eval_steps_per_second": 1.181,
      "step": 96
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 2026.24560546875,
      "learning_rate": 8e-06,
      "loss": 239.0,
      "step": 97
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 2510.28515625,
      "learning_rate": 8e-06,
      "loss": 253.0,
      "step": 98
    },
    {
      "epoch": 0.66,
      "grad_norm": 2243.331787109375,
      "learning_rate": 8e-06,
      "loss": 274.0,
      "step": 99
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2335.052734375,
      "learning_rate": 8e-06,
      "loss": 268.0,
      "step": 100
    },
    {
      "epoch": 0.6666666666666666,
      "eval_loss": 270.7466735839844,
      "eval_runtime": 31.9517,
      "eval_samples_per_second": 18.778,
      "eval_steps_per_second": 1.189,
      "step": 100
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 2334.329345703125,
      "learning_rate": 8e-06,
      "loss": 245.0,
      "step": 101
    },
    {
      "epoch": 0.68,
      "grad_norm": 2369.40576171875,
      "learning_rate": 8e-06,
      "loss": 174.0,
      "step": 102
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 2123.633056640625,
      "learning_rate": 8e-06,
      "loss": 199.0,
      "step": 103
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1781.3216552734375,
      "learning_rate": 8e-06,
      "loss": 175.0,
      "step": 104
    },
    {
      "epoch": 0.6933333333333334,
      "eval_loss": 266.9466552734375,
      "eval_runtime": 31.9782,
      "eval_samples_per_second": 18.763,
      "eval_steps_per_second": 1.188,
      "step": 104
    },
    {
      "epoch": 0.7,
      "grad_norm": 2318.53271484375,
      "learning_rate": 8e-06,
      "loss": 270.0,
      "step": 105
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 1889.259521484375,
      "learning_rate": 8e-06,
      "loss": 195.0,
      "step": 106
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 1740.31298828125,
      "learning_rate": 8e-06,
      "loss": 202.0,
      "step": 107
    },
    {
      "epoch": 0.72,
      "grad_norm": 2212.421142578125,
      "learning_rate": 8e-06,
      "loss": 274.0,
      "step": 108
    },
    {
      "epoch": 0.72,
      "eval_loss": 264.5733337402344,
      "eval_runtime": 31.9734,
      "eval_samples_per_second": 18.766,
      "eval_steps_per_second": 1.188,
      "step": 108
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 2216.845947265625,
      "learning_rate": 8e-06,
      "loss": 239.0,
      "step": 109
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 2457.214599609375,
      "learning_rate": 8e-06,
      "loss": 278.0,
      "step": 110
    },
    {
      "epoch": 0.74,
      "grad_norm": 2152.299072265625,
      "learning_rate": 8e-06,
      "loss": 202.0,
      "step": 111
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 5521.0478515625,
      "learning_rate": 8e-06,
      "loss": 193.0,
      "step": 112
    },
    {
      "epoch": 0.7466666666666667,
      "eval_loss": 265.1600036621094,
      "eval_runtime": 32.0807,
      "eval_samples_per_second": 18.703,
      "eval_steps_per_second": 1.185,
      "step": 112
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 8988.2451171875,
      "learning_rate": 8e-06,
      "loss": 196.0,
      "step": 113
    },
    {
      "epoch": 0.76,
      "grad_norm": 2640.77734375,
      "learning_rate": 8e-06,
      "loss": 239.0,
      "step": 114
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 6590.4228515625,
      "learning_rate": 8e-06,
      "loss": 264.0,
      "step": 115
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 2414.7197265625,
      "learning_rate": 8e-06,
      "loss": 322.0,
      "step": 116
    },
    {
      "epoch": 0.7733333333333333,
      "eval_loss": 261.85333251953125,
      "eval_runtime": 31.9588,
      "eval_samples_per_second": 18.774,
      "eval_steps_per_second": 1.189,
      "step": 116
    },
    {
      "epoch": 0.78,
      "grad_norm": 1987.833251953125,
      "learning_rate": 8e-06,
      "loss": 228.0,
      "step": 117
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 2885.177490234375,
      "learning_rate": 8e-06,
      "loss": 216.0,
      "step": 118
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 6315.779296875,
      "learning_rate": 8e-06,
      "loss": 193.0,
      "step": 119
    },
    {
      "epoch": 0.8,
      "grad_norm": 2811.72021484375,
      "learning_rate": 8e-06,
      "loss": 248.0,
      "step": 120
    },
    {
      "epoch": 0.8,
      "eval_loss": 262.5733337402344,
      "eval_runtime": 31.8853,
      "eval_samples_per_second": 18.817,
      "eval_steps_per_second": 1.192,
      "step": 120
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 1717.4896240234375,
      "learning_rate": 8e-06,
      "loss": 186.0,
      "step": 121
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 3021.698486328125,
      "learning_rate": 8e-06,
      "loss": 382.0,
      "step": 122
    },
    {
      "epoch": 0.82,
      "grad_norm": 1614.257568359375,
      "learning_rate": 8e-06,
      "loss": 185.0,
      "step": 123
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 1350.9912109375,
      "learning_rate": 8e-06,
      "loss": 162.0,
      "step": 124
    },
    {
      "epoch": 0.8266666666666667,
      "eval_loss": 257.0400085449219,
      "eval_runtime": 31.9296,
      "eval_samples_per_second": 18.791,
      "eval_steps_per_second": 1.19,
      "step": 124
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1427.439697265625,
      "learning_rate": 8e-06,
      "loss": 186.0,
      "step": 125
    },
    {
      "epoch": 0.84,
      "grad_norm": 7768.06494140625,
      "learning_rate": 8e-06,
      "loss": 199.0,
      "step": 126
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 2388.190185546875,
      "learning_rate": 8e-06,
      "loss": 251.0,
      "step": 127
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 2068.569580078125,
      "learning_rate": 8e-06,
      "loss": 208.0,
      "step": 128
    },
    {
      "epoch": 0.8533333333333334,
      "eval_loss": 256.0133361816406,
      "eval_runtime": 31.8957,
      "eval_samples_per_second": 18.811,
      "eval_steps_per_second": 1.191,
      "step": 128
    },
    {
      "epoch": 0.86,
      "grad_norm": 2428.1728515625,
      "learning_rate": 8e-06,
      "loss": 294.0,
      "step": 129
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 1667.419677734375,
      "learning_rate": 8e-06,
      "loss": 175.0,
      "step": 130
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 1483.2418212890625,
      "learning_rate": 8e-06,
      "loss": 206.0,
      "step": 131
    },
    {
      "epoch": 0.88,
      "grad_norm": 1879.4630126953125,
      "learning_rate": 8e-06,
      "loss": 235.0,
      "step": 132
    },
    {
      "epoch": 0.88,
      "eval_loss": 254.1199951171875,
      "eval_runtime": 31.7886,
      "eval_samples_per_second": 18.875,
      "eval_steps_per_second": 1.195,
      "step": 132
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 1808.83349609375,
      "learning_rate": 8e-06,
      "loss": 227.0,
      "step": 133
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 6465.81005859375,
      "learning_rate": 8e-06,
      "loss": 178.0,
      "step": 134
    },
    {
      "epoch": 0.9,
      "grad_norm": 1434.8441162109375,
      "learning_rate": 8e-06,
      "loss": 181.0,
      "step": 135
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 3035.028076171875,
      "learning_rate": 8e-06,
      "loss": 334.0,
      "step": 136
    },
    {
      "epoch": 0.9066666666666666,
      "eval_loss": 252.16000366210938,
      "eval_runtime": 31.8626,
      "eval_samples_per_second": 18.831,
      "eval_steps_per_second": 1.193,
      "step": 136
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 1509.304443359375,
      "learning_rate": 8e-06,
      "loss": 232.0,
      "step": 137
    },
    {
      "epoch": 0.92,
      "grad_norm": 1750.4986572265625,
      "learning_rate": 8e-06,
      "loss": 166.0,
      "step": 138
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 1796.85888671875,
      "learning_rate": 8e-06,
      "loss": 272.0,
      "step": 139
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1623.0107421875,
      "learning_rate": 8e-06,
      "loss": 245.0,
      "step": 140
    },
    {
      "epoch": 0.9333333333333333,
      "eval_loss": 248.3333282470703,
      "eval_runtime": 31.9257,
      "eval_samples_per_second": 18.794,
      "eval_steps_per_second": 1.19,
      "step": 140
    },
    {
      "epoch": 0.94,
      "grad_norm": 1799.5162353515625,
      "learning_rate": 8e-06,
      "loss": 274.0,
      "step": 141
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 1484.6806640625,
      "learning_rate": 8e-06,
      "loss": 189.0,
      "step": 142
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 1860.4530029296875,
      "learning_rate": 8e-06,
      "loss": 245.0,
      "step": 143
    },
    {
      "epoch": 0.96,
      "grad_norm": 110060.234375,
      "learning_rate": 8e-06,
      "loss": 262.0,
      "step": 144
    },
    {
      "epoch": 0.96,
      "eval_loss": 247.72000122070312,
      "eval_runtime": 31.7763,
      "eval_samples_per_second": 18.882,
      "eval_steps_per_second": 1.196,
      "step": 144
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 3225.373291015625,
      "learning_rate": 8e-06,
      "loss": 428.0,
      "step": 145
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 2324.75,
      "learning_rate": 8e-06,
      "loss": 346.0,
      "step": 146
    },
    {
      "epoch": 0.98,
      "grad_norm": 1963.406982421875,
      "learning_rate": 8e-06,
      "loss": 224.0,
      "step": 147
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 2568.467529296875,
      "learning_rate": 8e-06,
      "loss": 245.0,
      "step": 148
    },
    {
      "epoch": 0.9866666666666667,
      "eval_loss": 245.77333068847656,
      "eval_runtime": 31.9102,
      "eval_samples_per_second": 18.803,
      "eval_steps_per_second": 1.191,
      "step": 148
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 5844.57568359375,
      "learning_rate": 8e-06,
      "loss": 252.0,
      "step": 149
    },
    {
      "epoch": 1.0,
      "grad_norm": 1482.80908203125,
      "learning_rate": 8e-06,
      "loss": 201.0,
      "step": 150
    },
    {
      "epoch": 1.0066666666666666,
      "grad_norm": 1396.1539306640625,
      "learning_rate": 8e-06,
      "loss": 182.0,
      "step": 151
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 1283.587890625,
      "learning_rate": 8e-06,
      "loss": 164.0,
      "step": 152
    },
    {
      "epoch": 1.0133333333333334,
      "eval_loss": 244.3333282470703,
      "eval_runtime": 31.9312,
      "eval_samples_per_second": 18.79,
      "eval_steps_per_second": 1.19,
      "step": 152
    },
    {
      "epoch": 1.02,
      "grad_norm": 1480.92626953125,
      "learning_rate": 8e-06,
      "loss": 185.0,
      "step": 153
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 1731.8271484375,
      "learning_rate": 8e-06,
      "loss": 244.0,
      "step": 154
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 2860.17578125,
      "learning_rate": 8e-06,
      "loss": 222.0,
      "step": 155
    },
    {
      "epoch": 1.04,
      "grad_norm": 2508.63232421875,
      "learning_rate": 8e-06,
      "loss": 188.0,
      "step": 156
    },
    {
      "epoch": 1.04,
      "eval_loss": 242.77333068847656,
      "eval_runtime": 31.9111,
      "eval_samples_per_second": 18.802,
      "eval_steps_per_second": 1.191,
      "step": 156
    },
    {
      "epoch": 1.0466666666666666,
      "grad_norm": 2142.564453125,
      "learning_rate": 8e-06,
      "loss": 190.0,
      "step": 157
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 1556.25634765625,
      "learning_rate": 8e-06,
      "loss": 200.0,
      "step": 158
    },
    {
      "epoch": 1.06,
      "grad_norm": 2333.956787109375,
      "learning_rate": 8e-06,
      "loss": 228.0,
      "step": 159
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1628.6231689453125,
      "learning_rate": 8e-06,
      "loss": 158.0,
      "step": 160
    },
    {
      "epoch": 1.0666666666666667,
      "eval_loss": 240.77333068847656,
      "eval_runtime": 31.9276,
      "eval_samples_per_second": 18.793,
      "eval_steps_per_second": 1.19,
      "step": 160
    },
    {
      "epoch": 1.0733333333333333,
      "grad_norm": 2365.00244140625,
      "learning_rate": 8e-06,
      "loss": 206.0,
      "step": 161
    },
    {
      "epoch": 1.08,
      "grad_norm": 1848.728515625,
      "learning_rate": 8e-06,
      "loss": 192.0,
      "step": 162
    },
    {
      "epoch": 1.0866666666666667,
      "grad_norm": 1583.4525146484375,
      "learning_rate": 8e-06,
      "loss": 251.0,
      "step": 163
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 1571.895263671875,
      "learning_rate": 8e-06,
      "loss": 238.0,
      "step": 164
    },
    {
      "epoch": 1.0933333333333333,
      "eval_loss": 239.0800018310547,
      "eval_runtime": 31.8737,
      "eval_samples_per_second": 18.824,
      "eval_steps_per_second": 1.192,
      "step": 164
    },
    {
      "epoch": 1.1,
      "grad_norm": 1791.9246826171875,
      "learning_rate": 8e-06,
      "loss": 146.0,
      "step": 165
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 1416.0614013671875,
      "learning_rate": 8e-06,
      "loss": 174.0,
      "step": 166
    },
    {
      "epoch": 1.1133333333333333,
      "grad_norm": 2020.5830078125,
      "learning_rate": 8e-06,
      "loss": 298.0,
      "step": 167
    },
    {
      "epoch": 1.12,
      "grad_norm": 1760.1173095703125,
      "learning_rate": 8e-06,
      "loss": 164.0,
      "step": 168
    },
    {
      "epoch": 1.12,
      "eval_loss": 238.77333068847656,
      "eval_runtime": 31.9657,
      "eval_samples_per_second": 18.77,
      "eval_steps_per_second": 1.189,
      "step": 168
    },
    {
      "epoch": 1.1266666666666667,
      "grad_norm": 966.3563232421875,
      "learning_rate": 8e-06,
      "loss": 106.0,
      "step": 169
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 4202.1708984375,
      "learning_rate": 8e-06,
      "loss": 157.0,
      "step": 170
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 3401.103271484375,
      "learning_rate": 8e-06,
      "loss": 247.0,
      "step": 171
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 1465.6400146484375,
      "learning_rate": 8e-06,
      "loss": 158.0,
      "step": 172
    },
    {
      "epoch": 1.1466666666666667,
      "eval_loss": 239.25332641601562,
      "eval_runtime": 31.9753,
      "eval_samples_per_second": 18.764,
      "eval_steps_per_second": 1.188,
      "step": 172
    },
    {
      "epoch": 1.1533333333333333,
      "grad_norm": 1858.680419921875,
      "learning_rate": 8e-06,
      "loss": 258.0,
      "step": 173
    },
    {
      "epoch": 1.16,
      "grad_norm": 1049.5550537109375,
      "learning_rate": 8e-06,
      "loss": 96.0,
      "step": 174
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 1732.1484375,
      "learning_rate": 8e-06,
      "loss": 125.0,
      "step": 175
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 817.65771484375,
      "learning_rate": 8e-06,
      "loss": 107.0,
      "step": 176
    },
    {
      "epoch": 1.1733333333333333,
      "eval_loss": 237.22666931152344,
      "eval_runtime": 31.8917,
      "eval_samples_per_second": 18.814,
      "eval_steps_per_second": 1.192,
      "step": 176
    },
    {
      "epoch": 1.18,
      "grad_norm": 1148.545166015625,
      "learning_rate": 8e-06,
      "loss": 156.0,
      "step": 177
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 1555.2379150390625,
      "learning_rate": 8e-06,
      "loss": 147.0,
      "step": 178
    },
    {
      "epoch": 1.1933333333333334,
      "grad_norm": 1828.0184326171875,
      "learning_rate": 8e-06,
      "loss": 190.0,
      "step": 179
    },
    {
      "epoch": 1.2,
      "grad_norm": 1900.6649169921875,
      "learning_rate": 8e-06,
      "loss": 204.0,
      "step": 180
    },
    {
      "epoch": 1.2,
      "eval_loss": 237.01333618164062,
      "eval_runtime": 31.8465,
      "eval_samples_per_second": 18.84,
      "eval_steps_per_second": 1.193,
      "step": 180
    },
    {
      "epoch": 1.2066666666666666,
      "grad_norm": 1225.794677734375,
      "learning_rate": 8e-06,
      "loss": 137.0,
      "step": 181
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 1400.6181640625,
      "learning_rate": 8e-06,
      "loss": 170.0,
      "step": 182
    },
    {
      "epoch": 1.22,
      "grad_norm": 2631.382568359375,
      "learning_rate": 8e-06,
      "loss": 180.0,
      "step": 183
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 1899.6976318359375,
      "learning_rate": 8e-06,
      "loss": 232.0,
      "step": 184
    },
    {
      "epoch": 1.2266666666666666,
      "eval_loss": 236.22666931152344,
      "eval_runtime": 31.8701,
      "eval_samples_per_second": 18.826,
      "eval_steps_per_second": 1.192,
      "step": 184
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 2857.723388671875,
      "learning_rate": 8e-06,
      "loss": 158.0,
      "step": 185
    },
    {
      "epoch": 1.24,
      "grad_norm": 1616.6920166015625,
      "learning_rate": 8e-06,
      "loss": 235.0,
      "step": 186
    },
    {
      "epoch": 1.2466666666666666,
      "grad_norm": 1598.3065185546875,
      "learning_rate": 8e-06,
      "loss": 148.0,
      "step": 187
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 3134.509765625,
      "learning_rate": 8e-06,
      "loss": 225.0,
      "step": 188
    },
    {
      "epoch": 1.2533333333333334,
      "eval_loss": 236.18666076660156,
      "eval_runtime": 31.9062,
      "eval_samples_per_second": 18.805,
      "eval_steps_per_second": 1.191,
      "step": 188
    },
    {
      "epoch": 1.26,
      "grad_norm": 1156.1851806640625,
      "learning_rate": 8e-06,
      "loss": 121.5,
      "step": 189
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1581.065185546875,
      "learning_rate": 8e-06,
      "loss": 244.0,
      "step": 190
    },
    {
      "epoch": 1.2733333333333334,
      "grad_norm": 1833.0953369140625,
      "learning_rate": 8e-06,
      "loss": 238.0,
      "step": 191
    },
    {
      "epoch": 1.28,
      "grad_norm": 1219.5546875,
      "learning_rate": 8e-06,
      "loss": 166.0,
      "step": 192
    },
    {
      "epoch": 1.28,
      "eval_loss": 234.0800018310547,
      "eval_runtime": 31.8888,
      "eval_samples_per_second": 18.815,
      "eval_steps_per_second": 1.192,
      "step": 192
    },
    {
      "epoch": 1.2866666666666666,
      "grad_norm": 1617.0606689453125,
      "learning_rate": 8e-06,
      "loss": 155.0,
      "step": 193
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 4971.3525390625,
      "learning_rate": 8e-06,
      "loss": 229.0,
      "step": 194
    },
    {
      "epoch": 1.3,
      "grad_norm": 1177.0870361328125,
      "learning_rate": 8e-06,
      "loss": 147.0,
      "step": 195
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 1655.415771484375,
      "learning_rate": 8e-06,
      "loss": 198.0,
      "step": 196
    },
    {
      "epoch": 1.3066666666666666,
      "eval_loss": 235.27999877929688,
      "eval_runtime": 32.2823,
      "eval_samples_per_second": 18.586,
      "eval_steps_per_second": 1.177,
      "step": 196
    },
    {
      "epoch": 1.3133333333333335,
      "grad_norm": 1439.6358642578125,
      "learning_rate": 8e-06,
      "loss": 180.0,
      "step": 197
    },
    {
      "epoch": 1.32,
      "grad_norm": 968.3069458007812,
      "learning_rate": 8e-06,
      "loss": 120.0,
      "step": 198
    },
    {
      "epoch": 1.3266666666666667,
      "grad_norm": 1295.9091796875,
      "learning_rate": 8e-06,
      "loss": 162.0,
      "step": 199
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1749.804443359375,
      "learning_rate": 8e-06,
      "loss": 190.0,
      "step": 200
    },
    {
      "epoch": 1.3333333333333333,
      "eval_loss": 233.89332580566406,
      "eval_runtime": 31.8598,
      "eval_samples_per_second": 18.833,
      "eval_steps_per_second": 1.193,
      "step": 200
    },
    {
      "epoch": 1.34,
      "grad_norm": 3139.592529296875,
      "learning_rate": 8e-06,
      "loss": 237.0,
      "step": 201
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 1338.88037109375,
      "learning_rate": 8e-06,
      "loss": 155.0,
      "step": 202
    },
    {
      "epoch": 1.3533333333333333,
      "grad_norm": 2451.1142578125,
      "learning_rate": 8e-06,
      "loss": 268.0,
      "step": 203
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1618.5343017578125,
      "learning_rate": 8e-06,
      "loss": 209.0,
      "step": 204
    },
    {
      "epoch": 1.3599999999999999,
      "eval_loss": 233.18666076660156,
      "eval_runtime": 31.8845,
      "eval_samples_per_second": 18.818,
      "eval_steps_per_second": 1.192,
      "step": 204
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 1418.056396484375,
      "learning_rate": 8e-06,
      "loss": 146.0,
      "step": 205
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 1245.0614013671875,
      "learning_rate": 8e-06,
      "loss": 153.0,
      "step": 206
    },
    {
      "epoch": 1.38,
      "grad_norm": 1708.0660400390625,
      "learning_rate": 8e-06,
      "loss": 179.0,
      "step": 207
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 917.73779296875,
      "learning_rate": 8e-06,
      "loss": 111.5,
      "step": 208
    },
    {
      "epoch": 1.3866666666666667,
      "eval_loss": 231.1999969482422,
      "eval_runtime": 31.9932,
      "eval_samples_per_second": 18.754,
      "eval_steps_per_second": 1.188,
      "step": 208
    },
    {
      "epoch": 1.3933333333333333,
      "grad_norm": 1304.3453369140625,
      "learning_rate": 8e-06,
      "loss": 177.0,
      "step": 209
    },
    {
      "epoch": 1.4,
      "grad_norm": 1252.574951171875,
      "learning_rate": 8e-06,
      "loss": 103.0,
      "step": 210
    },
    {
      "epoch": 1.4066666666666667,
      "grad_norm": 1427.1929931640625,
      "learning_rate": 8e-06,
      "loss": 154.0,
      "step": 211
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 1597.9207763671875,
      "learning_rate": 8e-06,
      "loss": 237.0,
      "step": 212
    },
    {
      "epoch": 1.4133333333333333,
      "eval_loss": 229.60000610351562,
      "eval_runtime": 31.9005,
      "eval_samples_per_second": 18.809,
      "eval_steps_per_second": 1.191,
      "step": 212
    },
    {
      "epoch": 1.42,
      "grad_norm": 1171.9537353515625,
      "learning_rate": 8e-06,
      "loss": 168.0,
      "step": 213
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 1437.3135986328125,
      "learning_rate": 8e-06,
      "loss": 166.0,
      "step": 214
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 1760.99853515625,
      "learning_rate": 8e-06,
      "loss": 255.0,
      "step": 215
    },
    {
      "epoch": 1.44,
      "grad_norm": 1533.9791259765625,
      "learning_rate": 8e-06,
      "loss": 188.0,
      "step": 216
    },
    {
      "epoch": 1.44,
      "eval_loss": 228.82666015625,
      "eval_runtime": 31.7558,
      "eval_samples_per_second": 18.894,
      "eval_steps_per_second": 1.197,
      "step": 216
    },
    {
      "epoch": 1.4466666666666668,
      "grad_norm": 1624.3404541015625,
      "learning_rate": 8e-06,
      "loss": 221.0,
      "step": 217
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 1360.548583984375,
      "learning_rate": 8e-06,
      "loss": 181.0,
      "step": 218
    },
    {
      "epoch": 1.46,
      "grad_norm": 4919.77294921875,
      "learning_rate": 8e-06,
      "loss": 211.0,
      "step": 219
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 2496.70849609375,
      "learning_rate": 8e-06,
      "loss": 130.0,
      "step": 220
    },
    {
      "epoch": 1.4666666666666668,
      "eval_loss": 227.63999938964844,
      "eval_runtime": 31.8969,
      "eval_samples_per_second": 18.811,
      "eval_steps_per_second": 1.191,
      "step": 220
    },
    {
      "epoch": 1.4733333333333334,
      "grad_norm": 3806.88720703125,
      "learning_rate": 8e-06,
      "loss": 183.0,
      "step": 221
    },
    {
      "epoch": 1.48,
      "grad_norm": 1873.4886474609375,
      "learning_rate": 8e-06,
      "loss": 239.0,
      "step": 222
    },
    {
      "epoch": 1.4866666666666668,
      "grad_norm": 2575.287841796875,
      "learning_rate": 8e-06,
      "loss": 161.0,
      "step": 223
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 1511.8916015625,
      "learning_rate": 8e-06,
      "loss": 219.0,
      "step": 224
    },
    {
      "epoch": 1.4933333333333334,
      "eval_loss": 226.3866729736328,
      "eval_runtime": 31.7914,
      "eval_samples_per_second": 18.873,
      "eval_steps_per_second": 1.195,
      "step": 224
    },
    {
      "epoch": 1.5,
      "grad_norm": 940.9728393554688,
      "learning_rate": 8e-06,
      "loss": 131.0,
      "step": 225
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 2887.889892578125,
      "learning_rate": 8e-06,
      "loss": 192.0,
      "step": 226
    },
    {
      "epoch": 1.5133333333333332,
      "grad_norm": 1896.59912109375,
      "learning_rate": 8e-06,
      "loss": 149.0,
      "step": 227
    },
    {
      "epoch": 1.52,
      "grad_norm": 1653.5435791015625,
      "learning_rate": 8e-06,
      "loss": 232.0,
      "step": 228
    },
    {
      "epoch": 1.52,
      "eval_loss": 225.74667358398438,
      "eval_runtime": 31.8569,
      "eval_samples_per_second": 18.834,
      "eval_steps_per_second": 1.193,
      "step": 228
    },
    {
      "epoch": 1.5266666666666666,
      "grad_norm": 1691.9871826171875,
      "learning_rate": 8e-06,
      "loss": 330.0,
      "step": 229
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 1813.0345458984375,
      "learning_rate": 8e-06,
      "loss": 255.0,
      "step": 230
    },
    {
      "epoch": 1.54,
      "grad_norm": 1460.181640625,
      "learning_rate": 8e-06,
      "loss": 198.0,
      "step": 231
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 1314.8470458984375,
      "learning_rate": 8e-06,
      "loss": 181.0,
      "step": 232
    },
    {
      "epoch": 1.5466666666666666,
      "eval_loss": 224.44000244140625,
      "eval_runtime": 31.8865,
      "eval_samples_per_second": 18.817,
      "eval_steps_per_second": 1.192,
      "step": 232
    },
    {
      "epoch": 1.5533333333333332,
      "grad_norm": 1326.16552734375,
      "learning_rate": 8e-06,
      "loss": 175.0,
      "step": 233
    },
    {
      "epoch": 1.56,
      "grad_norm": 1290.763671875,
      "learning_rate": 8e-06,
      "loss": 197.0,
      "step": 234
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 1229.488525390625,
      "learning_rate": 8e-06,
      "loss": 174.0,
      "step": 235
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 1211.1341552734375,
      "learning_rate": 8e-06,
      "loss": 175.0,
      "step": 236
    },
    {
      "epoch": 1.5733333333333333,
      "eval_loss": 223.2133331298828,
      "eval_runtime": 31.8245,
      "eval_samples_per_second": 18.853,
      "eval_steps_per_second": 1.194,
      "step": 236
    },
    {
      "epoch": 1.58,
      "grad_norm": 2158.182861328125,
      "learning_rate": 8e-06,
      "loss": 145.0,
      "step": 237
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 2426.379150390625,
      "learning_rate": 8e-06,
      "loss": 172.0,
      "step": 238
    },
    {
      "epoch": 1.5933333333333333,
      "grad_norm": 1038.4154052734375,
      "learning_rate": 8e-06,
      "loss": 128.0,
      "step": 239
    },
    {
      "epoch": 1.6,
      "grad_norm": 1817.6295166015625,
      "learning_rate": 8e-06,
      "loss": 216.0,
      "step": 240
    },
    {
      "epoch": 1.6,
      "eval_loss": 222.9600067138672,
      "eval_runtime": 31.8837,
      "eval_samples_per_second": 18.818,
      "eval_steps_per_second": 1.192,
      "step": 240
    },
    {
      "epoch": 1.6066666666666667,
      "grad_norm": 1595.4898681640625,
      "learning_rate": 8e-06,
      "loss": 205.0,
      "step": 241
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 1222.9100341796875,
      "learning_rate": 8e-06,
      "loss": 171.0,
      "step": 242
    },
    {
      "epoch": 1.62,
      "grad_norm": 1513.1756591796875,
      "learning_rate": 8e-06,
      "loss": 235.0,
      "step": 243
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 1476.6134033203125,
      "learning_rate": 8e-06,
      "loss": 219.0,
      "step": 244
    },
    {
      "epoch": 1.6266666666666667,
      "eval_loss": 221.32000732421875,
      "eval_runtime": 31.8746,
      "eval_samples_per_second": 18.824,
      "eval_steps_per_second": 1.192,
      "step": 244
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 1719.9354248046875,
      "learning_rate": 8e-06,
      "loss": 260.0,
      "step": 245
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1210.858642578125,
      "learning_rate": 8e-06,
      "loss": 146.0,
      "step": 246
    },
    {
      "epoch": 1.6466666666666665,
      "grad_norm": 1348.9530029296875,
      "learning_rate": 8e-06,
      "loss": 152.0,
      "step": 247
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 819.3543090820312,
      "learning_rate": 8e-06,
      "loss": 97.0,
      "step": 248
    },
    {
      "epoch": 1.6533333333333333,
      "eval_loss": 220.6266632080078,
      "eval_runtime": 31.9111,
      "eval_samples_per_second": 18.802,
      "eval_steps_per_second": 1.191,
      "step": 248
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1424.997314453125,
      "learning_rate": 8e-06,
      "loss": 167.0,
      "step": 249
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2378.991943359375,
      "learning_rate": 8e-06,
      "loss": 224.0,
      "step": 250
    },
    {
      "epoch": 1.6733333333333333,
      "grad_norm": 1518.170654296875,
      "learning_rate": 8e-06,
      "loss": 165.0,
      "step": 251
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 923.0525512695312,
      "learning_rate": 8e-06,
      "loss": 118.0,
      "step": 252
    },
    {
      "epoch": 1.6800000000000002,
      "eval_loss": 220.74667358398438,
      "eval_runtime": 31.8391,
      "eval_samples_per_second": 18.845,
      "eval_steps_per_second": 1.194,
      "step": 252
    },
    {
      "epoch": 1.6866666666666665,
      "grad_norm": 1649.8056640625,
      "learning_rate": 8e-06,
      "loss": 255.0,
      "step": 253
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 1666.286865234375,
      "learning_rate": 8e-06,
      "loss": 183.0,
      "step": 254
    },
    {
      "epoch": 1.7,
      "grad_norm": 10748.3857421875,
      "learning_rate": 8e-06,
      "loss": 173.0,
      "step": 255
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 2500.14599609375,
      "learning_rate": 8e-06,
      "loss": 214.0,
      "step": 256
    },
    {
      "epoch": 1.7066666666666666,
      "eval_loss": 219.9066619873047,
      "eval_runtime": 31.9002,
      "eval_samples_per_second": 18.809,
      "eval_steps_per_second": 1.191,
      "step": 256
    },
    {
      "epoch": 1.7133333333333334,
      "grad_norm": 2763.03857421875,
      "learning_rate": 8e-06,
      "loss": 209.0,
      "step": 257
    },
    {
      "epoch": 1.72,
      "grad_norm": 1378.843017578125,
      "learning_rate": 8e-06,
      "loss": 180.0,
      "step": 258
    },
    {
      "epoch": 1.7266666666666666,
      "grad_norm": 4142.640625,
      "learning_rate": 8e-06,
      "loss": 217.0,
      "step": 259
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 1415.7254638671875,
      "learning_rate": 8e-06,
      "loss": 204.0,
      "step": 260
    },
    {
      "epoch": 1.7333333333333334,
      "eval_loss": 220.63999938964844,
      "eval_runtime": 31.8716,
      "eval_samples_per_second": 18.826,
      "eval_steps_per_second": 1.192,
      "step": 260
    },
    {
      "epoch": 1.74,
      "grad_norm": 1225.316162109375,
      "learning_rate": 8e-06,
      "loss": 195.0,
      "step": 261
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 1190.7724609375,
      "learning_rate": 8e-06,
      "loss": 169.0,
      "step": 262
    },
    {
      "epoch": 1.7533333333333334,
      "grad_norm": 1024.1690673828125,
      "learning_rate": 8e-06,
      "loss": 140.0,
      "step": 263
    },
    {
      "epoch": 1.76,
      "grad_norm": 1003.3812866210938,
      "learning_rate": 8e-06,
      "loss": 116.0,
      "step": 264
    },
    {
      "epoch": 1.76,
      "eval_loss": 218.93333435058594,
      "eval_runtime": 31.82,
      "eval_samples_per_second": 18.856,
      "eval_steps_per_second": 1.194,
      "step": 264
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 1549.574462890625,
      "learning_rate": 8e-06,
      "loss": 166.0,
      "step": 265
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 946.2348022460938,
      "learning_rate": 8e-06,
      "loss": 135.0,
      "step": 266
    },
    {
      "epoch": 1.78,
      "grad_norm": 1068.1558837890625,
      "learning_rate": 8e-06,
      "loss": 120.5,
      "step": 267
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 1573.65185546875,
      "learning_rate": 8e-06,
      "loss": 166.0,
      "step": 268
    },
    {
      "epoch": 1.7866666666666666,
      "eval_loss": 218.6133270263672,
      "eval_runtime": 31.8705,
      "eval_samples_per_second": 18.826,
      "eval_steps_per_second": 1.192,
      "step": 268
    },
    {
      "epoch": 1.7933333333333334,
      "grad_norm": 1270.253173828125,
      "learning_rate": 8e-06,
      "loss": 134.0,
      "step": 269
    },
    {
      "epoch": 1.8,
      "grad_norm": 4062.162353515625,
      "learning_rate": 8e-06,
      "loss": 177.0,
      "step": 270
    },
    {
      "epoch": 1.8066666666666666,
      "grad_norm": 2055.971435546875,
      "learning_rate": 8e-06,
      "loss": 144.0,
      "step": 271
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 7700.435546875,
      "learning_rate": 8e-06,
      "loss": 159.0,
      "step": 272
    },
    {
      "epoch": 1.8133333333333335,
      "eval_loss": 220.82666015625,
      "eval_runtime": 31.8923,
      "eval_samples_per_second": 18.813,
      "eval_steps_per_second": 1.192,
      "step": 272
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1420.695556640625,
      "learning_rate": 8e-06,
      "loss": 198.0,
      "step": 273
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 1506.1109619140625,
      "learning_rate": 8e-06,
      "loss": 178.0,
      "step": 274
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 1336.0545654296875,
      "learning_rate": 8e-06,
      "loss": 224.0,
      "step": 275
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1302.890380859375,
      "learning_rate": 8e-06,
      "loss": 187.0,
      "step": 276
    },
    {
      "epoch": 1.8399999999999999,
      "eval_loss": 218.3333282470703,
      "eval_runtime": 31.8983,
      "eval_samples_per_second": 18.81,
      "eval_steps_per_second": 1.191,
      "step": 276
    },
    {
      "epoch": 1.8466666666666667,
      "grad_norm": 1179.56787109375,
      "learning_rate": 8e-06,
      "loss": 122.0,
      "step": 277
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 8577.564453125,
      "learning_rate": 8e-06,
      "loss": 195.0,
      "step": 278
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1090.416015625,
      "learning_rate": 8e-06,
      "loss": 123.5,
      "step": 279
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1369.853271484375,
      "learning_rate": 8e-06,
      "loss": 181.0,
      "step": 280
    },
    {
      "epoch": 1.8666666666666667,
      "eval_loss": 218.7066650390625,
      "eval_runtime": 31.8981,
      "eval_samples_per_second": 18.81,
      "eval_steps_per_second": 1.191,
      "step": 280
    },
    {
      "epoch": 1.8733333333333333,
      "grad_norm": 1557.4981689453125,
      "learning_rate": 8e-06,
      "loss": 176.0,
      "step": 281
    },
    {
      "epoch": 1.88,
      "grad_norm": 2054.255859375,
      "learning_rate": 8e-06,
      "loss": 170.0,
      "step": 282
    },
    {
      "epoch": 1.8866666666666667,
      "grad_norm": 3047.3818359375,
      "learning_rate": 8e-06,
      "loss": 183.0,
      "step": 283
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 1914.101806640625,
      "learning_rate": 8e-06,
      "loss": 152.0,
      "step": 284
    },
    {
      "epoch": 1.8933333333333333,
      "eval_loss": 218.34666442871094,
      "eval_runtime": 31.9494,
      "eval_samples_per_second": 18.78,
      "eval_steps_per_second": 1.189,
      "step": 284
    },
    {
      "epoch": 1.9,
      "grad_norm": 912.2167358398438,
      "learning_rate": 8e-06,
      "loss": 96.0,
      "step": 285
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 1463.962646484375,
      "learning_rate": 8e-06,
      "loss": 188.0,
      "step": 286
    },
    {
      "epoch": 1.9133333333333333,
      "grad_norm": 1510.1591796875,
      "learning_rate": 8e-06,
      "loss": 191.0,
      "step": 287
    },
    {
      "epoch": 1.92,
      "grad_norm": 1587.2138671875,
      "learning_rate": 8e-06,
      "loss": 170.0,
      "step": 288
    },
    {
      "epoch": 1.92,
      "eval_loss": 217.25332641601562,
      "eval_runtime": 32.074,
      "eval_samples_per_second": 18.707,
      "eval_steps_per_second": 1.185,
      "step": 288
    },
    {
      "epoch": 1.9266666666666667,
      "grad_norm": 1713.7451171875,
      "learning_rate": 8e-06,
      "loss": 196.0,
      "step": 289
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 2034.8912353515625,
      "learning_rate": 8e-06,
      "loss": 171.0,
      "step": 290
    },
    {
      "epoch": 1.94,
      "grad_norm": 1332.1358642578125,
      "learning_rate": 8e-06,
      "loss": 175.0,
      "step": 291
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 1871.83349609375,
      "learning_rate": 8e-06,
      "loss": 205.0,
      "step": 292
    },
    {
      "epoch": 1.9466666666666668,
      "eval_loss": 214.9066619873047,
      "eval_runtime": 31.8754,
      "eval_samples_per_second": 18.823,
      "eval_steps_per_second": 1.192,
      "step": 292
    },
    {
      "epoch": 1.9533333333333334,
      "grad_norm": 1091.8876953125,
      "learning_rate": 8e-06,
      "loss": 133.0,
      "step": 293
    },
    {
      "epoch": 1.96,
      "grad_norm": 4014.925048828125,
      "learning_rate": 8e-06,
      "loss": 172.0,
      "step": 294
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 1350.8548583984375,
      "learning_rate": 8e-06,
      "loss": 184.0,
      "step": 295
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 1503.8782958984375,
      "learning_rate": 8e-06,
      "loss": 235.0,
      "step": 296
    },
    {
      "epoch": 1.9733333333333334,
      "eval_loss": 214.3866729736328,
      "eval_runtime": 31.9688,
      "eval_samples_per_second": 18.768,
      "eval_steps_per_second": 1.189,
      "step": 296
    },
    {
      "epoch": 1.98,
      "grad_norm": 3922.2001953125,
      "learning_rate": 8e-06,
      "loss": 243.0,
      "step": 297
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 1366.5040283203125,
      "learning_rate": 8e-06,
      "loss": 172.0,
      "step": 298
    },
    {
      "epoch": 1.9933333333333332,
      "grad_norm": 1442.2596435546875,
      "learning_rate": 8e-06,
      "loss": 200.0,
      "step": 299
    },
    {
      "epoch": 2.0,
      "grad_norm": 1784.7947998046875,
      "learning_rate": 8e-06,
      "loss": 203.0,
      "step": 300
    },
    {
      "epoch": 2.0,
      "eval_loss": 214.7066650390625,
      "eval_runtime": 31.8863,
      "eval_samples_per_second": 18.817,
      "eval_steps_per_second": 1.192,
      "step": 300
    },
    {
      "epoch": 2.0,
      "step": 300,
      "total_flos": 2.9622536559919104e+16,
      "train_loss": 267.2,
      "train_runtime": 4606.9942,
      "train_samples_per_second": 1.042,
      "train_steps_per_second": 0.065
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 4,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.9622536559919104e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
